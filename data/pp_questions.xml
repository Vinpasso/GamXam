<?xml version="1.0" encoding="UTF-8" ?>

<exam xmlns="https://vpt1.org"
      xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
      xsi:schemaLocation="https://vpt1.org questions.xsd">
    <head>
        <title>Parallel Programming</title>
        <version>1.0</version>
        <id>161</id>
    </head>
    <body>
        <!-- TODO: Insert questions -->
        <!-- Lecture 2: Threading Concepts Threading APIs POSIX Threads -->
        <question type="extended-answer" id="1">
            <body>What are the four elements of Flynn's taxonomy?</body>
            <answers>
                <answer>
                    <body>SISD (Conventional processor), SIMD (Single instruction multiple data, vectorization, GPU,
                        etc.), MISD (not really available), MIMD (Multiple instruction multiple data,
                        multi-core/processor etc.)
                    </body>
                    <marks>
                        <mark type="keyword">sisd</mark>
                        <mark type="keyword">simd</mark>
                        <mark type="keyword">misd</mark>
                        <mark type="keyword">mimd</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="2">
            <body>What two major types of MIMD architectures exist?</body>
            <answers>
                <answer>
                    <body>Distributed memory and shared memory</body>
                    <marks>
                        <mark type="keyword">distrib</mark>
                        <mark type="keyword">share</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="3">
            <body>What type of distributed memory systems exist and what do they do?</body>
            <answers>
                <answer>
                    <body>MPP, NOW, Cluster (What do they do? TODO)</body>
                    <marks>
                        <mark type="keyword">mpp</mark>
                        <mark type="keyword">now</mark>
                        <mark type="keyword">clust</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="4">
            <body>What are the types and subtypes of shared memory systems?</body>
            <answers>
                <answer>
                    <body>UMA and NUMA (COMA, ccNUMA, nccNUMA)</body>
                    <marks>
                        <mark type="keyword">UMA</mark>
                        <mark type="keyword">NUMA</mark>
                        <mark type="keyword">COMA</mark>
                        <mark type="keyword">cc</mark>
                        <mark type="keyword">ncc</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="5">
            <body>What is another name for the UMA Architecture? What is the defining feature of a UMA system?</body>
            <answers>
                <answer>
                    <body>SMP: Symmetric multiprocessors. There is a centralized shared memory, to which all processors
                        have the same latency.
                    </body>
                    <marks>
                        <mark type="keyword">smp</mark>
                        <mark type="keyword">shared</mark>
                        <mark type="keyword">latenc</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="6">
            <body>What are some programming models that match shared memory?</body>
            <answers>
                <answer>
                    <body>
                        POSIX threads, OpenMP, etc.
                    </body>
                    <marks>
                        <mark type="keyword">posix</mark>
                        <mark type="regex">o(pen)?mp</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="7">
            <body>What is a thread and what are its essential components?</body>
            <answers>
                <answer>
                    <body>An independent stream of execution that has at least its own PC and its own stack.</body>
                    <marks>
                        <mark type="keyword">exec</mark>
                        <mark type="keyword">pc</mark>
                        <mark type="keyword">stack</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="8">
            <body>What is Hyperthreading/SMT?</body>
            <answers>
                <answer>
                    <body>A technology where a processing core can have multiple hardware threads to allow for rapid
                        context switching in the core, reducing the impact of stalls.
                    </body>
                    <marks>
                        <mark type="keyword">thread</mark>
                        <mark type="keyword">switch</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="9">
            <body>What are some advantages of using threads versus multiple processes.</body>
            <answers>
                <answer>
                    <body>
                        No data protection boundaries, asynchronous behavior within a process, OS still does scheduling
                        (preemption and progress)
                    </body>
                    <marks>
                        <mark type="keyword">protect</mark>
                        <mark type="keyword">async</mark>
                        <mark type="keyword">schedul</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10">
            <body>What is an advantage of using threads (e.g. pthreads) in parallel programming?</body>
            <answers>
                <answer>
                    <body>Its low-level, allowing the programmer to micro-manage resources.</body>
                    <marks>
                        <mark type="keyword">low</mark>
                        <mark type="keyword">resource</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="11">
            <body>What model does pthreads conform to?</body>
            <answers>
                <answer>
                    <body>The fork/join model</body>
                    <marks>
                        <mark type="keyword">fork</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="12">
            <body>Write code to create/destroy a pthread.</body>
            <answers>
                <answer>
                    <body>
                        Using a kernel:
                        <pre>void* kernel(void* arg);</pre>
                        Start a thread:
                        <pre>
                            pthread_t thread;
                            pthread_attr_t attr;
                            if(!pthread_create(&amp;thread, attr, kernel, arg)) {
                            abort(0);
                            }
                        </pre>
                        Join a thread:
                        <pre>
                            if(!pthread_join(&amp;thread, &amp;retval)) {
                            abort(0);
                            }
                        </pre>
                    </body>
                    <marks>
                        <mark type="keyword">create</mark>
                        <mark type="keyword">join</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="13">
            <body>What is a potential issue when using pthreads concerning memory?</body>
            <answers>
                <answer>
                    <body>The stacks of multiple threads need to be stored in the same address space. Usually, this is
                        done with a pre-defined space between each stack, limiting the amount of available stack space.
                    </body>
                    <marks>
                        <mark type="keyword">stack</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="14">
            <body>What are the two main concepts for synchronization between threads in POSIX?</body>
            <answers>
                <answer>
                    <body>Mutual exclusion, condition variables</body>
                    <marks>
                        <mark type="keyword">excl</mark>
                        <mark type="keyword">condit</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="15">
            <body>Which problem is solved by the mutual exclusion concept?</body>
            <answers>
                <answer>
                    <body>Concurrent access to shared resources (IO, Memory, etc.)</body>
                    <marks>
                        <mark type="keyword">resourc</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="16">
            <body>Write code for one lifecycle of a POSIX Mutex.</body>
            <answers>
                <answer>
                    <body>
                        Initialization (to use default attributes, use NULL):
                        <pre>
                            //Dynamic case, destruction necessary
                            pthread_mutex_t lock;
                            pthread_mutex_init(&amp;lock, attr);
                            //Static case
                            pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;
                        </pre>
                        Lock:
                        <pre>
                            pthread_mutex_lock(&amp;lock);
                            pthread_mutex_trylock(&amp;lock);
                        </pre>
                        Unlock:
                        <pre>
                            pthread_mutex_unlock(&amp;lock);
                        </pre>
                        Destroy:
                        <pre>
                            pthread_mutex_destroy(&amp;lock);
                        </pre>
                    </body>
                    <marks></marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="17">
            <body>What types of recursive behaviors are available for pthread mutexes?</body>
            <answers>
                <answer>
                    <body>PTHREAD_MUTEX_NORMAL (deadlock), PTHREAD_MUTEX_ERRORCHECK (error code),
                        PTHREAD_MUTEX_RECURSIVE (lock count). Default: undefined behavior
                    </body>
                    <marks>
                        <mark type="keyword">normal</mark>
                        <mark type="keyword">error</mark>
                        <mark type="keyword">recur</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="18">
            <body>What criteria are there for the implementation of a pthread mutex?</body>
            <answers>
                <answer>
                    <body>Guarantee of mutual exclusion, progress (every waiting thread eventually gets the mutex),
                        fairness.
                    </body>
                    <marks>
                        <mark type="keyword">excl</mark>
                        <mark type="keyword">prog</mark>
                        <mark type="keyword">fair</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="19">
            <body>What are the two typical implementations for a pthread mutex and what do they do?</body>
            <answers>
                <answer>
                    <body>
                        Spin-lock: Try to obtain lock in a loop as fast as possible, enable low latency but consuming
                        resources.
                        <p/>
                        Yielding lock: Yields the hardware thread when lock unavailable, freeing resources but
                        increasing latency.
                    </body>
                    <marks>
                        <mark type="keyword">spin</mark>
                        <mark type="keyword">yield</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="20">
            <body>What implementations of a mutex exist with/without hardware support?</body>
            <answers>
                <answer>
                    <body>HW Support: Locking using atomic operations, such as test and set or compare and swap. No HW
                        Support: Using e.g. Petersen's algorithm.
                    </body>
                    <marks>
                        <mark type="keyword">atomic</mark>
                        <mark type="keyword">petersen</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="21">
            <body>What is the tradeoff with lock granularity?</body>
            <answers>
                <answer>
                    <body>Too fine and a significant portion of compute time will be spent on obtaining and releasing
                        locks. Too coarse and threads will need to wait for accessing resources that could be accessed
                        concurrently.
                    </body>
                    <marks>
                        <mark type="keyword">time</mark>
                        <mark type="keyword">resourc</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="22">
            <body>When does a deadlock occur?</body>
            <answers>
                <answer>
                    <body>When there is a cyclic dependency when obtaining locks.</body>
                    <marks>
                        <mark type="keyword">cycl</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="23">
            <body>What approaches exist to resolve the deadlock problem and what is a drawback of each?</body>
            <answers>
                <answer>
                    <body>One lock (very coarse). Arbiter (complex central instance, scalability). Lock order
                        (fairness). Custom/hybrid schemes.
                    </body>
                    <marks>
                        <mark type="keyword">one</mark>
                        <mark type="keyword">arbit</mark>
                        <mark type="keyword">order</mark>
                        <mark type="keyword">hybrid</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="24">
            <body>What is the concept behind condition variables for synchronization?</body>
            <answers>
                <answer>
                    <body>One thread blocks while the other thread sends a signal once the condition is met.</body>
                    <marks>
                        <mark type="keyword">block</mark>
                        <mark type="keyword">signal</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="25">
            <body>How are pthread condition variables used (write one lifecycle of code)?</body>
            <answers>
                <answer>
                    <body>
                        Lifecycle:
                        <pre>
                            //Static
                            pthread_mutex_t mutex = PTHREAD_MUTEX_INITIALIZER;
                            pthread_cond_t cond = PTHREAD_COND_INITIALIZER;

                            ...
                            //Waiting Thread
                            pthread_mutex_lock(&amp;lock);
                            //Do work
                            // Make sure to guard against sporadic wakes
                            while(!CONDITION) {
                            // This unlocks and relocks mutex implicitly so other threads have access
                            pthread_cond_wait(&amp;cond, &amp;mutex);
                            }
                            //Do work
                            pthread_mutex_unlock(&amp;lock);

                            ...
                            //Signal thread
                            pthread_mutex_lock(&amp;lock);
                            //Work
                            CONDITION = true;
                            pthread_cond_signal(&amp;cond);
                            pthread_mutex_unlock(&amp;mutex);
                        </pre>
                    </body>
                    <marks>
                        <mark type="keyword">lock</mark>
                        <mark type="keyword">wait</mark>
                        <mark type="keyword">signal</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="26">
            <body>What are some relevant performance aspects of threading?</body>
            <answers>
                <answer>
                    <body>Overhead of thread management (creation/destruction, use thread pools). Lock contention with
                        many threads. Pinning of threads. Good use of cache and avoiding false sharing.
                    </body>
                    <marks>
                        <mark type="keyword">thread</mark>
                        <mark type="keyword">lock</mark>
                        <mark type="keyword">pin</mark>
                        <mark type="keyword">cache</mark>
                        <mark type="keyword">shar</mark>
                    </marks>
                </answer>
            </answers>
        </question>

        <!-- Chapter 3 -->
        <question type="extended-answer" id="27">
            <body>What were the design goals of OpenMP?</body>
            <answers>
                <answer>
                    <body>Standard for writing parallel programs (mainly on-node), "lean and mean" (simple API for
                        complex goals), ease of use and portability
                    </body>
                    <marks>
                        <mark type="keyword">standard</mark>
                        <mark type="keyword">lean</mark>
                        <mark type="keyword">ease</mark>
                        <mark type="keyword">port</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="28">
            <body>What is OpenMP?</body>
            <answers>
                <answer>
                    <body>An Application Program Interface (API) used to program multi-threaded shared memory
                        parallelism. It is comprised of 3 components: compiler directives, a runtime library, and
                        environment variables.
                    </body>
                    <marks>
                        <mark type="keyword">API</mark>
                        <mark type="keyword">share</mark>
                        <mark type="keyword">component</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="29">
            <body>How does OpenMP correspond to the Fork/Join Model?</body>
            <answers>
                <answer>
                    <body>
                        Step 1: Sequential execution.
                        Step 2: Begin of parallel region, spawning threads
                        Step 3: Parallel computation.
                        Step 4: Synchronization at end of parallel region (implicit/explicit)
                        Step 5: Sequential execution.
                    </body>
                    <marks>
                        <mark type="keyword">region</mark>
                        <mark type="keyword">sequen</mark>
                        <mark type="keyword">parall</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="30">
            <body>What are some misconceptions about nested parallel regions?</body>
            <answers>
                <answer>
                    <body>OpenMP is not required to use more threads (can just continue executing sequentially), mapping
                        to HW threads is runtime dependent.
                    </body>
                    <marks>
                        <mark type="keyword">sequen</mark>
                        <mark type="keyword">HW</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="31">
            <body>What is needed for an OpenMP program?</body>
            <answers>
                <answer>
                    <body>A compiler with OpenMP support as well as a runtime system (usually based on pthreads).
                        Behavior can be controlled by ICVs (internal control variables).
                    </body>
                    <marks>
                        <mark type="keyword">compile</mark>
                        <mark type="keyword">runtime</mark>
                        <mark type="keyword">variable</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="32">
            <body>What do OpenMP compiler directives apply to?</body>
            <answers>
                <answer>
                    <body>A structured block ("{}") with a single entry and exit point.</body>
                    <marks>
                        <mark type="keyword">block</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="33">
            <body>Which OpenMP directive is used to make sure multiple pieces of code are executed once by one thread in
                parallel? Where is the synchronization point?
            </body>
            <answers>
                <answer>
                    <body>A section:
                        <pre>
                            0 #pragma omp parallel
                            1 {
                            2 #pragma omp sections
                            3 {
                            4 #pragma omp section
                            5 { block }
                            6 #pragma omp section
                            7 { block }
                            8 }
                            9 }
                        </pre>
                        Implicit barrier at the end of a sections block.
                    </body>
                    <marks>
                        <mark type="keyword">section</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="34">
            <body>Which OpenMP directive is used to parallelize pieces of a loop? What condition needs to hold and are
                there synchronization points?
            </body>
            <answers>
                <answer>
                    <body>
                        Inside a parallel region, use the following to distribute iterations across threads:
                        <pre>#pragma omp for</pre>
                        Condition: All iterations must be independent. Synchronization happens implicitly at end of
                        loop.
                    </body>
                    <marks>
                        <mark type="keyword">for</mark>
                        <mark type="keyword">independen</mark>
                        <mark type="keyword">sync</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <!-- TODO: Continue Chapter 3 Slide 26 -->


        <!-- From Exam 2018 -->
        <question type="extended-answer" id="1000">
            <body>1a) What can be computed by applying Amdahl's Law [and state the law]?</body>
            <answers>
                <answer>
                    <body>Using Amdahl's Law, we can compute the minimum achievable runtime in a program
                        consisting of sequential and parallel regions for a given number of processors.
                        Amdahl's Law states that <pre>T(p)= (1-f)T + f(T/p)</pre>, where f is fraction of parallel
                        execution and p number of processes. From this, it is possible to derive the maximum speedup
                        S(p)=1/(1-f+f/p).
                    </body>
                    <marks>
                        <mark type="keyword">speedup</mark>
                        <mark type="keyword">sequen</mark>
                        <mark type="keyword">par</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10001">
            <body>1b) Assume you have a program, which is split into regions that either cannot be parallelized at all
                or that can be parallelized perfectly. Assuming the regions that cannot be parallelized consume 1% of
                execution time during a sequential run, what is the maximal speedup that can be achieved?
            </body>
            <answers>
                <answer>
                    <body>Using Amdahl's Law with f=0.99, the maximum achievable speedup is 1/(1-f+f/p), with p very
                        large. Thus 1/(1-0.99) = 1/0.01 = 100.
                    </body>
                    <marks>
                        <mark type="keyword">100</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10002">
            <body>2a) Name one possible reason for a negative performance impact in case lock granularity is chosen too
                fine.
            </body>
            <answers>
                <answer>
                    <body>When lock granularity is too fine there are many lock/unlock operations when accessing
                        resources, the processing of which detracts from the time spent on useful computation.
                    </body>
                    <marks>
                        <mark type="keyword">lock</mark>
                        <mark type="keyword">time</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10003">
            <body>2b) Name one possible reason for a negative performance impact in case lock granularity is chosen too
                coarse:
            </body>
            <answers>
                <answer>
                    <body>When lock granularity is too coarse, accessing independent resources (such as different
                        locations in a large array) can block just because they are protected by the same lock (for the
                        whole array). This requires a process to wait even though the resource could be accessed.
                    </body>
                    <marks>
                        <mark type="keyword">lock</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10004">
            <body>3a) Parallelize for loop in the following function using OpenMP by replacing &gt;OPENMP HERE&lt;with
                the appropriate OpenMP directive(s) and clause(s). Write your solution in the box below.
                <pre>
                    void function(float* a,float* b,int n) {
                    int i;
                    &lt;OPENMP HERE&gt;
                    for(i = 0; i &lt; n - 1; i++)
                    a[i] = (b[i] + b[i + 1]) / 2;
                    a[i] = b[i] / 2;
                    }
                </pre>
            </body>
            <answers>
                <answer>
                    <body>
                        <pre>
                            #pragma omp parallel for private(i) shared(a, b, n)
                        </pre>
                    </body>
                    <marks>
                        <mark type="keyword">private</mark>
                        <mark type="keyword">shared</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10005">
            <body>b)* Parallelize for loop in the following function using OpenMP by replacing &gt;OPENMP HERE&lt; with
                the appropriate OpenMP directive(s) and clause(s). Write your solution in the box below.
                <pre>
                    double function(float* x,int* y,int n)
                    {
                    int i, b = y[0];
                    float a = 1.0;
                    &gt;OPENMP HERE&lt;
                    for(i=0; i &gt; n; i++) {
                    a += x[i];
                    if(b &lt; y[i])
                    b = y[i];
                    }
                    return a * b;
                    }
                </pre>
            </body>
            <answers>
                <answer>
                    <body>#pragma omp parallel for private(i) reduction(+: a) reduction(min: b) shared(x, y)</body>
                    <marks>
                        <mark type="keyword">reduction</mark>
                        <mark type="keyword">+</mark>
                        <mark type="keyword">min</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10006">
            <body>
                Consider the following program being executed with OMP_NUM_THREADS set to 16.
                <pre>
                    int value[64], k, t;
                    for(inti=0; i&lt;64; i++)
                    value[i]=63-i;
                    t=omp_get_thread_num ();
                    k=42;
                    #pragma omp parallel for schedule(static, 2) CLAUSE
                    for(inti=0; i&lt;64; i++) {
                    k=value[i]*2+t;
                    }
                    printf("Final␣value␣of␣k=%i\n", k);
                </pre>
                a)* Which iterations are executed by thread 13 in the parallel for loop?
            </body>
            <answers>
                <answer>
                    <body>Iterations 26, 27 and 58, 59</body>
                    <marks>
                        <mark type="keyword">26</mark>
                        <mark type="keyword">27</mark>
                        <mark type="keyword">58</mark>
                        <mark type="keyword">59</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10007">
            <body>
                Consider the following program being executed with OMP_NUM_THREADS set to 16.
                <pre>
                    int value[64], k, t;
                    for(inti=0; i&lt;64; i++)
                    value[i]=63-i;
                    t=omp_get_thread_num ();
                    k=42;
                    #pragma omp parallel for schedule(static, 2) CLAUSE
                    for(inti=0; i&lt;64; i++) {
                    k=value[i]*2+t;
                    }
                    printf("Final␣value␣of␣k=%i\n", k);
                </pre>
                b)* Fill in the following table. If multiple values are possible, state MULTIPLE.
                <pre>
                    CLAUSE set to | Printed value of k
                    NONE |
                    private(k,t) |
                    shared(k,t) |
                    firstprivate(k,t) |
                    lastprivate(k,t) |
                    firstprivate(k,t) |
                    lastprivate(k,t) |
                </pre>
            </body>
            <answers>
                <answer>
                    <body>
                        <pre>
                            CLAUSE set to | Printed value of k
                            NONE | MULTIPLE (default shared)
                            private(k,t) | 42
                            shared(k,t) | MULTIPLE
                            firstprivate(k,t) | 42
                            lastprivate(k,t) | MULTIPLE (due to uninitialized, observed only 0)
                            firstprivate(k,t) lastprivate(k,t) | 0
                        </pre>
                    </body>
                    <marks>
                        <mark type="keyword">MULTIPLE</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10008">
            <body>
                a)* List the OpenMP pragma used to specify a task [and the clause to ensure a task completes]:
            </body>
            <answers>
                <answer>
                    <body>
                        <pre>
                            #pragma omp task
                            #pragma omp taskwait
                        </pre>
                    </body>
                    <marks>
                        <mark type="keyword">task</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10009">
            <body>b)* What is the difference between a tied and an untied task?</body>
            <answers>
                <answer>
                    <body>A tied task is guaranteed to be started and completed by the single thread, while an untied
                        task can be migrate between threads during its execution.
                    </body>
                    <marks>
                        <mark type="keyword">thread</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10010">
            <body>a)* Name three potential performance drawbacks in the following code snippet if it is executed on a
                two-socket node.
                <pre>
                    void main() {
                    const int N = 1000000;
                    int a[N];
                    int b[N];
                    for(int i = 0; i &gt; N; i++) {
                    a[i] = i;
                    }
                    #pragma omp parallel for schedule(static, 1)
                    for(int i = 0; i &gt; N; i++) {
                    a[i] = a[i] + 17;
                    b[i] = a[i] % 23;
                    }
                    }
                </pre>
            </body>
            <answers>
                <answer>
                    <body>
                        1: Memory is not initialized on the thread where it is used, causing poor performance on NUMA
                        systems with first-touch.
                        <p/>
                        2: False Sharing may occur because multiple processes are writing to and reading from memory
                        addresses that have close proximity in memory.
                        <p/>
                        3: TODO (options: load imbalance due to static scheduling, RAW dependency inside loop,
                        microscheduling)
                    </body>
                    <marks>
                        <mark type="keyword">init</mark>
                        <mark type="keyword">false</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10011">
            <body>a)* What conditions must hold for a program to be sequentially consistent?</body>
            <answers>
                <answer>
                    <body>
                        As stated in lecture:
                        <pre>
                            [Lamport] “A multiprocessor system is sequentially consistent if the result of any execution
                            is the same as if the operations of all processors were executed in some sequential order,
                            and the operations of each individual processor appear in this sequence in the order
                            specified by the program”
                        </pre>
                    </body>
                    <marks>
                        <mark type="keyword">order</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10012">
            <body>a)* What is the difference between an eager and a rendevouz protocol?</body>
            <answers>
                <answer>
                    <body>In an eager protocol, the message gets sent directly. This avoids a handshake but might add
                        extra copy operations into/out of unexpected message queues, suitable for short messages. With
                        the rendezvous protocol, only a header is sent. Once the receive has been posted, a message to
                        fetch the actual message is posted. This avoids extra copying at the cost of one additional
                        round-trip, suited for large messages where copying is expensive.
                    </body>
                    <marks>
                        <mark type="keyword">handshake</mark>
                        <mark type="keyword">copy</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10013">
            <body>b)* Describe the functionality of MPI_Waitany</body>
            <answers>
                <answer>
                    <body>When passed an array of MPI_Request objects and a MPI_Status object, MPI_Waitany blocks until
                        any one of the passed requests completes, filling the status variable with status information.
                    </body>
                    <marks>
                        <mark type="keyword">block</mark>
                        <mark type="keyword">request</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10014">
            <body>a)* Describe the functionality of MPI_Comm_split().</body>
            <answers>
                <answer>
                    <body>MPI_Comm_split(originalComm, color (group), key (order), newComm). Splits the current
                        communicator into one or more new communicators, with processes passing the same color value
                        assigned to the same new communicator, where the key determines the order of the processes in
                        the new communicator. The result is stored in newComm.
                    </body>
                    <marks>
                        <mark type="keyword">color</mark>
                        <mark type="keyword">key</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <!-- TODO: Problem 9 B and C -->
        <question type="extended-answer" id="10015">
            <body>a)* List the blocking collective operation with its parameters that can be used to replace the point
                to point communication between lines 20 to 28.
                <pre>
                    19 // point to point communication
                    20 if(rank == 0)
                    21 for(int r = 1; r &lt; size; r++)
                    22 {
                    23 a[0] = data;
                    24 MPI_Recv(a + r, 1, MPI_FLOAT , r, 0, MPI_COMM_WORLD ,25MPI_STATUS_IGNORE );
                    26 }
                    27 else
                    28 MPI_Send (&amp;data , 1, MPI_FLOAT , 0, 0, MPI_COMM_WORLD );
                </pre>
            </body>
            <answers>
                <answer>
                    <body>
                        Instead of point to point, you can use:
                        <pre>
                            MPI_Gather(&amp;data, 1, MPI_FLOAT, &amp;a, 1 MPI_FLOAT, 0, MPI_COMM_WORLD);
                        </pre>
                    </body>
                    <marks>
                        <mark type="keyword">gather</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10016">
            <body>b)* List the non-blocking collective operation with its parameters that can be used to replace the
                blocking communication from previous subproblem (a) (MPI_Gather).
            </body>
            <answers>
                <answer>
                    <body>
                        Instead of blocking, you can use:
                        <pre>
                            MPI_Request request;
                            MPI_Igather(&amp;data, 1, MPI_FLOAT, &amp;a, 1 MPI_FLOAT, 0, MPI_COMM_WORLD, &amp;request);
                        </pre>
                    </body>
                    <marks>
                        <mark type="keyword">Igather</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10017">
            <body>c)* Name the appropriate operation when using a non-blocking collective to insure its completion and
                specify the proper line number to put this operation for getting the best performance.
                <img src="../data/res/pp/10017_code.png"/>
            </body>
            <answers>
                <answer>
                    <body>
                        Assuming that the MPI_Request is stored in a variable named request:
                        <pre>
                            MPI_Wait(&amp;request, &amp;MPI_STATUS_IGNORE);
                        </pre>
                        This should be placed on line 31.
                    </body>
                    <marks>
                        <mark type="keyword">31</mark>
                        <mark type="keyword">MPI_Wait</mark>
                    </marks>
                </answer>
            </answers>
        </question>
    </body>
    <!-- TODO: Problem 11 -->
</exam>