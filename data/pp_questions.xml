<?xml version="1.0" encoding="UTF-8" ?>

<exam xmlns="https://vpt1.org"
      xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
      xsi:schemaLocation="https://vpt1.org questions.xsd">
    <head>
        <title>Parallel Programming</title>
        <version>1.0</version>
        <id>161</id>
    </head>
    <body>
        <!-- TODO: Insert questions -->
        <!-- Lecture 2: Threading Concepts Threading APIs POSIX Threads -->
        <question type="extended-answer" id="1">
            <body>What are the four elements of Flynn's taxonomy?</body>
            <answers>
                <answer>
                    <body>SISD (Conventional processor), SIMD (Single instruction multiple data, vectorization, GPU,
                        etc.), MISD (not really available), MIMD (Multiple instruction multiple data,
                        multi-core/processor etc.)
                    </body>
                    <marks>
                        <mark type="keyword">sisd</mark>
                        <mark type="keyword">simd</mark>
                        <mark type="keyword">misd</mark>
                        <mark type="keyword">mimd</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="2">
            <body>What two major types of MIMD architectures exist?</body>
            <answers>
                <answer>
                    <body>Distributed memory and shared memory</body>
                    <marks>
                        <mark type="keyword">distrib</mark>
                        <mark type="keyword">share</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="3">
            <body>What type of distributed memory systems exist and what do they do?</body>
            <answers>
                <answer>
                    <body>MPP, NOW, Cluster (What do they do? TODO)</body>
                    <marks>
                        <mark type="keyword">mpp</mark>
                        <mark type="keyword">now</mark>
                        <mark type="keyword">clust</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="4">
            <body>What are the types and subtypes of shared memory systems?</body>
            <answers>
                <answer>
                    <body>UMA and NUMA (COMA, ccNUMA, nccNUMA)</body>
                    <marks>
                        <mark type="keyword">UMA</mark>
                        <mark type="keyword">NUMA</mark>
                        <mark type="keyword">COMA</mark>
                        <mark type="keyword">cc</mark>
                        <mark type="keyword">ncc</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="5">
            <body>What is another name for the UMA Architecture? What is the defining feature of a UMA system?</body>
            <answers>
                <answer>
                    <body>SMP: Symmetric multiprocessors. There is a centralized shared memory, to which all processors
                        have the same latency.
                    </body>
                    <marks>
                        <mark type="keyword">smp</mark>
                        <mark type="keyword">shared</mark>
                        <mark type="keyword">latenc</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="6">
            <body>What are some programming models that match shared memory?</body>
            <answers>
                <answer>
                    <body>
                        POSIX threads, OpenMP, etc.
                    </body>
                    <marks>
                        <mark type="keyword">posix</mark>
                        <mark type="regex">o(pen)?mp</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="7">
            <body>What is a thread and what are its essential components?</body>
            <answers>
                <answer>
                    <body>An independent stream of execution that has at least its own PC and its own stack.</body>
                    <marks>
                        <mark type="keyword">exec</mark>
                        <mark type="keyword">pc</mark>
                        <mark type="keyword">stack</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="8">
            <body>What is Hyperthreading/SMT?</body>
            <answers>
                <answer>
                    <body>A technology where a processing core can have multiple hardware threads to allow for rapid
                        context switching in the core, reducing the impact of stalls.
                    </body>
                    <marks>
                        <mark type="keyword">thread</mark>
                        <mark type="keyword">switch</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="9">
            <body>What are some advantages of using threads versus multiple processes.</body>
            <answers>
                <answer>
                    <body>
                        No data protection boundaries, asynchronous behavior within a process, OS still does scheduling
                        (preemption and progress)
                    </body>
                    <marks>
                        <mark type="keyword">protect</mark>
                        <mark type="keyword">async</mark>
                        <mark type="keyword">schedul</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10">
            <body>What is an advantage of using threads (e.g. pthreads) in parallel programming?</body>
            <answers>
                <answer>
                    <body>Its low-level, allowing the programmer to micro-manage resources.</body>
                    <marks>
                        <mark type="keyword">low</mark>
                        <mark type="keyword">resource</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="11">
            <body>What model does pthreads conform to?</body>
            <answers>
                <answer>
                    <body>The fork/join model</body>
                    <marks>
                        <mark type="keyword">fork</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="12">
            <body>Write code to create/destroy a pthread.</body>
            <answers>
                <answer>
                    <body>
                        Using a kernel:
                        <pre>
                            void* kernel(void* arg);
                        </pre>
                        Start a thread:
                        <pre>
                            <!-- @formatter:off -->
                            pthread_t thread;
                            pthread_attr_t attr;
                            //Note: The signature is pthread_create(pthread_t* thread, pthread_attr* attr, void* (*kernel)(void* arg), void* arg);
                            // However, the kernel does not need to be referenced.
                            if(!pthread_create(&amp;thread, attr, kernel, arg)) {
                                abort(0);
                            }
                            <!-- @formatter:on -->
                        </pre>
                        Join a thread:
                        <pre>
                            <!-- @formatter:off -->
                            if(!pthread_join(&amp;thread, &amp;retval)) {
                                abort(0);
                            }
                            <!-- @formatter:on -->
                        </pre>
                    </body>
                    <marks>
                        <mark type="keyword">create</mark>
                        <mark type="keyword">join</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="13">
            <body>What is a potential issue when using pthreads concerning memory?</body>
            <answers>
                <answer>
                    <body>The stacks of multiple threads need to be stored in the same address space. Usually, this is
                        done with a pre-defined space between each stack, limiting the amount of available stack space.
                    </body>
                    <marks>
                        <mark type="keyword">stack</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="14">
            <body>What are the two main concepts for synchronization between threads in POSIX?</body>
            <answers>
                <answer>
                    <body>Mutual exclusion, condition variables</body>
                    <marks>
                        <mark type="keyword">excl</mark>
                        <mark type="keyword">condit</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="15">
            <body>Which problem is solved by the mutual exclusion concept?</body>
            <answers>
                <answer>
                    <body>Concurrent access to shared resources (IO, Memory, etc.)</body>
                    <marks>
                        <mark type="keyword">resourc</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="16">
            <body>Write code for one lifecycle of a POSIX Mutex.</body>
            <answers>
                <answer>
                    <body>
                        Initialization (to use default attributes, use NULL):
                        <pre>
                            //Dynamic case, destruction necessary
                            pthread_mutex_t lock;
                            pthread_mutex_init(&amp;lock, attr);
                            //Static case
                            pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;
                        </pre>
                        Lock:
                        <pre>
                            pthread_mutex_lock(&amp;lock);
                            pthread_mutex_trylock(&amp;lock);
                        </pre>
                        Unlock:
                        <pre>
                            pthread_mutex_unlock(&amp;lock);
                        </pre>
                        Destroy:
                        <pre>
                            pthread_mutex_destroy(&amp;lock);
                        </pre>
                    </body>
                    <marks></marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="17">
            <body>What types of recursive behaviors are available for pthread mutexes?</body>
            <answers>
                <answer>
                    <body>PTHREAD_MUTEX_NORMAL (deadlock), PTHREAD_MUTEX_ERRORCHECK (error code),
                        PTHREAD_MUTEX_RECURSIVE (lock count). Default: undefined behavior
                    </body>
                    <marks>
                        <mark type="keyword">normal</mark>
                        <mark type="keyword">error</mark>
                        <mark type="keyword">recur</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="18">
            <body>What criteria are there for the implementation of a pthread mutex?</body>
            <answers>
                <answer>
                    <body>Guarantee of mutual exclusion, progress (every waiting thread eventually gets the mutex),
                        fairness.
                    </body>
                    <marks>
                        <mark type="keyword">excl</mark>
                        <mark type="keyword">prog</mark>
                        <mark type="keyword">fair</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="19">
            <body>What are the two typical implementations for a pthread mutex and what do they do?</body>
            <answers>
                <answer>
                    <body>
                        Spin-lock: Try to obtain lock in a loop as fast as possible, enable low latency but consuming
                        resources.
                        <p/>
                        Yielding lock: Yields the hardware thread when lock unavailable, freeing resources but
                        increasing latency.
                    </body>
                    <marks>
                        <mark type="keyword">spin</mark>
                        <mark type="keyword">yield</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="20">
            <body>What implementations of a mutex exist with/without hardware support?</body>
            <answers>
                <answer>
                    <body>HW Support: Locking using atomic operations, such as test and set or compare and swap. No HW
                        Support: Using e.g. Petersen's algorithm.
                    </body>
                    <marks>
                        <mark type="keyword">atomic</mark>
                        <mark type="keyword">petersen</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="21">
            <body>What is the tradeoff with lock granularity?</body>
            <answers>
                <answer>
                    <body>Too fine and a significant portion of compute time will be spent on obtaining and releasing
                        locks. Too coarse and threads will need to wait for accessing resources that could be accessed
                        concurrently.
                    </body>
                    <marks>
                        <mark type="keyword">time</mark>
                        <mark type="keyword">resourc</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="22">
            <body>When does a deadlock occur?</body>
            <answers>
                <answer>
                    <body>When there is a cyclic dependency when obtaining locks.</body>
                    <marks>
                        <mark type="keyword">cycl</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="23">
            <body>What approaches exist to resolve the deadlock problem and what is a drawback of each?</body>
            <answers>
                <answer>
                    <body>One lock (very coarse). Arbiter (complex central instance, scalability). Lock order
                        (fairness). Custom/hybrid schemes.
                    </body>
                    <marks>
                        <mark type="keyword">one</mark>
                        <mark type="keyword">arbit</mark>
                        <mark type="keyword">order</mark>
                        <mark type="keyword">hybrid</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="24">
            <body>What is the concept behind condition variables for synchronization?</body>
            <answers>
                <answer>
                    <body>One thread blocks while the other thread sends a signal once the condition is met.</body>
                    <marks>
                        <mark type="keyword">block</mark>
                        <mark type="keyword">signal</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="25">
            <body>How are pthread condition variables used (write one lifecycle of code)?</body>
            <answers>
                <answer>
                    <body>
                        Lifecycle:
                        <pre>
                            <!-- @formatter:off -->
                            //Static
                            pthread_mutex_t mutex = PTHREAD_MUTEX_INITIALIZER;
                            pthread_cond_t cond = PTHREAD_COND_INITIALIZER;

                            ...
                            //Waiting Thread
                                pthread_mutex_lock(&amp;lock);
                                //Do work
                                // Make sure to guard against sporadic wakes
                                while(!CONDITION) {
                                    // This unlocks and relocks mutex implicitly so other threads have access
                                    pthread_cond_wait(&amp;cond, &amp;mutex);
                                }
                                //Do work
                                pthread_mutex_unlock(&amp;lock);

                            ...
                            //Signal thread
                                pthread_mutex_lock(&amp;lock);
                                //Work
                                CONDITION = true;
                                pthread_cond_signal(&amp;cond);
                                pthread_mutex_unlock(&amp;mutex);
                            <!-- @formatter:on -->
                        </pre>
                    </body>
                    <marks>
                        <mark type="keyword">lock</mark>
                        <mark type="keyword">wait</mark>
                        <mark type="keyword">signal</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="26">
            <body>What are some relevant performance aspects of threading?</body>
            <answers>
                <answer>
                    <body>Overhead of thread management (creation/destruction, use thread pools). Lock contention with
                        many threads. Pinning of threads. Good use of cache and avoiding false sharing.
                    </body>
                    <marks>
                        <mark type="keyword">thread</mark>
                        <mark type="keyword">lock</mark>
                        <mark type="keyword">pin</mark>
                        <mark type="keyword">cache</mark>
                        <mark type="keyword">shar</mark>
                    </marks>
                </answer>
            </answers>
        </question>

        <!-- Chapter 3 -->
        <question type="extended-answer" id="27">
            <body>What were the design goals of OpenMP?</body>
            <answers>
                <answer>
                    <body>Standard for writing parallel programs (mainly on-node), "lean and mean" (simple API for
                        complex goals), ease of use and portability
                    </body>
                    <marks>
                        <mark type="keyword">standard</mark>
                        <mark type="keyword">lean</mark>
                        <mark type="keyword">ease</mark>
                        <mark type="keyword">port</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="28">
            <body>What is OpenMP?</body>
            <answers>
                <answer>
                    <body>An Application Program Interface (API) used to program multi-threaded shared memory
                        parallelism. It is comprised of 3 components: compiler directives, a runtime library, and
                        environment variables.
                    </body>
                    <marks>
                        <mark type="keyword">API</mark>
                        <mark type="keyword">share</mark>
                        <mark type="keyword">component</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="29">
            <body>How does OpenMP correspond to the Fork/Join Model?</body>
            <answers>
                <answer>
                    <body>
                        Step 1: Sequential execution.
                        Step 2: Begin of parallel region, spawning threads
                        Step 3: Parallel computation.
                        Step 4: Synchronization at end of parallel region (implicit/explicit)
                        Step 5: Sequential execution.
                    </body>
                    <marks>
                        <mark type="keyword">region</mark>
                        <mark type="keyword">sequen</mark>
                        <mark type="keyword">parall</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="30">
            <body>What are some misconceptions about nested parallel regions?</body>
            <answers>
                <answer>
                    <body>OpenMP is not required to use more threads (can just continue executing sequentially), mapping
                        to HW threads is runtime dependent.
                    </body>
                    <marks>
                        <mark type="keyword">sequen</mark>
                        <mark type="keyword">HW</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="31">
            <body>What is needed for an OpenMP program?</body>
            <answers>
                <answer>
                    <body>A compiler with OpenMP support as well as a runtime system (usually based on pthreads).
                        Behavior can be controlled by ICVs (internal control variables).
                    </body>
                    <marks>
                        <mark type="keyword">compile</mark>
                        <mark type="keyword">runtime</mark>
                        <mark type="keyword">variable</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="32">
            <body>What do OpenMP compiler directives apply to?</body>
            <answers>
                <answer>
                    <body>A structured block ("{}") with a single entry and exit point.</body>
                    <marks>
                        <mark type="keyword">block</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="33">
            <body>Which OpenMP directive is used to make sure multiple pieces of code are executed once by one thread in
                parallel? Where is the synchronization point?
            </body>
            <answers>
                <answer>
                    <body>A section:
                        <pre>
                            <!-- @formatter:off -->
                            0   #pragma omp parallel
                            1   {
                            2       #pragma omp sections
                            3       {
                            4           #pragma omp section
                            5           { block }
                            6           #pragma omp section
                            7           { block }
                            8       }
                            9   }
                            <!-- @formatter:on -->
                        </pre>
                        Implicit barrier at the end of a sections block.
                    </body>
                    <marks>
                        <mark type="keyword">section</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="34">
            <body>Which OpenMP directive is used to parallelize pieces of a loop? What condition needs to hold and are
                there synchronization points?
            </body>
            <answers>
                <answer>
                    <body>
                        Inside a parallel region, use the following to distribute iterations across threads:
                        <pre>#pragma omp for</pre>
                        Condition: All iterations must be independent. Synchronization happens implicitly at end of
                        loop.
                    </body>
                    <marks>
                        <mark type="keyword">for</mark>
                        <mark type="keyword">independen</mark>
                        <mark type="keyword">sync</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="35">
            <body>What are the available OpenMP Loop schedules and what do they do?</body>
            <answers>
                <answer>
                    <body>Static (preassigned, default size ~n/t), dynamic (fixed size, default 1), guided (chunks
                        decreasing in size), runtime (using environment variable), auto (let OpenMP choose).
                    </body>
                    <marks>
                        <mark type="keyword">static</mark>
                        <mark type="keyword">dynamic</mark>
                        <mark type="keyword">guide</mark>
                        <mark type="keyword">runtime</mark>
                        <mark type="keyword">auto</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="36">
            <body>What is the final value of i in the following code snippet and why?
                <pre>
                    <!-- @formatter:off -->
                    int i = 3;
                    #pragma omp parallel for lastprivate(i)
                    for(int j=0; j&lt;4; j++) {
                        i=i+1;
                        printf("-> i=%d\n", i);
                    }
                    printf("Final Value of I=%d\n", i);
                    <!-- @formatter:on -->
                </pre>
            </body>
            <answers>
                <answer>
                    <body>Undefined, because the private variable is not initialized.</body>
                    <marks>
                        <mark type="keyword">undef</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="37">
            <body>What is the behavior of the sharing attribute default(none)?</body>
            <answers>
                <answer>
                    <body>All variables need to be explicitly set to shared or private, no implicit sharing.</body>
                    <marks>
                        <mark type="keyword">explicit</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="38">
            <body>What are the available OpenMP reduction operators and how is a reduction written? What limitation is
                placed on the variable?
            </body>
            <answers>
                <answer>
                    <body>With the following clause:
                        <pre>
                            reduction(operator: list)
                        </pre>
                        Where operator is one of the following:
                        <pre>
                            arithmetic: +, *, - (alert: subtract in local part, then thread results added together),
                            min, max
                            logical: &amp;, ^, |, &amp;&amp;, ||
                            user defined (newer versions)
                        </pre>
                        The variable is only allowed to show up in the following operations:
                        <pre>
                            x = x operator expr
                            x operator= expr
                            x++, ++x, x--, --x
                        </pre>
                    </body>
                    <marks>
                        <mark type="keyword">reduc</mark>
                        <mark type="keyword">op</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="39">
            <body>What is the difference between an OpenMP master region, single region, critical section, and atomic
                statement?
            </body>
            <answers>
                <answer>
                    <body>
                        A master region enforces that only the (specific) master thread executes the code block (no
                        synchronization at beginning).
                        <p/>
                        A single region allows any one thread to execute the region, others skipping (implicit sync at
                        end)
                        <p/>
                        A critical region is a mutually exclusive block executed by all threads (use naming if multiple,
                        all unnamed mapped to same name.
                        <p/>
                        An atomic statement is like a one statement critical block (limited to x operator= expr,
                        x++, x--, ...) that may be implemented in hardware to avoid locking.
                    </body>
                    <marks>
                        <mark type="keyword">thread</mark>
                        <mark type="keyword">excl</mark>
                        <mark type="keyword">critical</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="40">
            <body>What constructs does OpenMP provide for locking?</body>
            <answers>
                <answer>
                    <body>Simple locks using omp_init_lock(&amp;lock), omp_destroy_lock(&amp;lock), omp_set_lock(&amp;lock),
                        omp_unset_lock(&amp;lock), res = omp_test_lock(&amp;lock) (checks and possibly sets lock,
                        returning true if acquired).
                        <p/>
                        Similar routines for nestable locks.
                    </body>
                    <marks>
                        <mark type="keyword">simple</mark>
                        <mark type="keyword">nestable</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="41">
            <body>What does the OpenMP ordered construct do and how is it used?</body>
            <answers>
                <answer>
                    <body>Ensures that the ordered block (S2) is executed in sequential order. Blocks outside that
                        region may float.
                        <pre>
                            <!-- @formatter:off -->
                            1   #pragma omp for ordered
                            2   for (...){
                            3       S1
                            4       #pragma omp ordered
                            5       {
                            6           S2
                            7       }
                            8       S3
                            9   }
                            <!-- @formatter:on -->
                        </pre>
                        <img src="../data/res/pp/41_sequence.png"/>
                    </body>
                    <marks>
                        <mark type="keyword">sequen</mark>
                    </marks>
                </answer>
            </answers>
        </question>

        <question type="extended-answer" id="42">
            <body>What is the difference between omp_get_max_threads() and omp_get_num_threads()?</body>
            <answers>
                <answer>
                    <body>omp_get_max_threads determines the maximum number of threads for team creation, omp get num
                        threads determines the number of threads in the current team (1 in sequential code).
                    </body>
                    <marks>
                        <mark type="keyword">team</mark>
                        <mark type="keyword">sequen</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="43">
            <body>What are some drawbacks of work sharing in OpenMP? What is a possible solution?</body>
            <answers>
                <answer>
                    <body>Imbalance caused by workload (especially static, NUMA), and machine (differences in
                        environment). Limited programming flexibility (sections/for loops) not suited for hierarchical
                        parallelism. Explicit tasking can be used as a solution.
                    </body>
                    <marks>
                        <mark type="keyword">work</mark>
                        <mark type="keyword">machine</mark>
                        <mark type="keyword">hierarch</mark>
                        <mark type="keyword">task</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="44">
            <body>What is a task in OpenMP?</body>
            <answers>
                <answer>
                    <body>An independent piece of work (that is guaranteed to be executed, in any order). They are
                        scheduled using task queues and dispatched to threads.
                    </body>
                    <marks>
                        <mark type="keyword">independen</mark>
                        <mark type="keyword">work</mark>
                        <mark type="keyword">order</mark>
                        <mark type="keyword">queue</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="45">
            <body>What is the difference between a tied and an untied task?</body>
            <answers>
                <answer>
                    <body>A tied task is bound to a thread once it starts and can no longer move, while an untied task
                        might be interrupted and moved elsewhere, offering more flexibility and resource utilization.
                    </body>
                    <marks>
                        <mark type="keyword">thread</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="46">
            <body>How can OpenMP tasks be used to split work?</body>
            <answers>
                <answer>
                    <body>
                        By using the task construct inside a parallel region. Make sure that you don't spawn more tasks
                        than intended (multiple threads).
                        <pre>
                            <!-- @formatter:off -->
                            #pragma omp parallel
                            {
                                #pragma omp single
                                {
                                    for ( elem = l->first; elem; elem= elem->next)
                                        #pragma omp task
                                        process(elem)
                                }
                                // all tasks are complete by this point
                            }
                            <!-- @formatter:on -->
                        </pre>
                    </body>
                    <marks>
                        <mark type="keyword">task</mark>
                        <mark type="keyword">single</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="47">
            <body>
                What does the following clause do if condition is true/false?
                <pre>
                    #pragma omp task if(CONDITION)
                    { ... }
                </pre>
            </body>
            <answers>
                <answer>
                    <body>If true, creates the task normally. If false, the creating thread executes the new task
                        immediately, and cannot resume the current task without completing the new one.
                    </body>
                    <marks>
                        <mark type="keyword">task</mark>
                        <mark type="keyword">thread</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="48">
            <body>
                What does the following clause do?
                <pre>
                    #pragma omp taskwait
                </pre>
            </body>
            <answers>
                <answer>
                    <body>Waits for the completion of immediate child tasks, where child tasks are tasks generated since
                        the beginning of the current task.
                    </body>
                    <marks>
                        <mark type="keyword">child</mark>
                        <mark type="keyword">task</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="49">
            <body>What are some explicit/implicit task scheduling points in OpenMP?</body>
            <answers>
                <answer>
                    <body>
                        Explicit (standalone directive):
                        <pre>
                            #pragma omp taskyield
                        </pre>
                        Implicit: task creation, end of task, taskwait, barrier
                    </body>
                    <marks>
                        <mark type="keyword">yield</mark>
                        <mark type="keyword">creat</mark>
                        <mark type="keyword">end</mark>
                        <mark type="keyword">wait</mark>
                        <mark type="keyword">barrier</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="50">
            <body>What 3 types of OpenMP constructs are there?</body>
            <answers>
                <answer>
                    <body>Worksharing constructs (loops, sections, single), tasking constructs (task, taskyield),
                        master/synchronization constructs (master, critical, barrier, taskwait, atomic, flush, ordered)
                    </body>
                    <marks>
                        <mark type="keyword">workshar</mark>
                        <mark type="keyword">task</mark>
                        <mark type="keyword">sync</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="51">
            <body>How are OpenMP task dependencies used, how do they work?</body>
            <answers>
                <answer>
                    <body>Using the depend clause:
                        <pre>#pragma omp task shared(...) depend(in/out/inout: var)</pre>
                        in: depends on all previously generated tasks with out/inout.
                        <p/>
                        out: depends on all previously generated tasks with in/out/inout
                        <p/>
                    </body>
                    <marks>
                        <mark type="keyword">depend</mark>
                        <mark type="keyword">in</mark>
                        <mark type="keyword">out</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="52">
            <body>What are some advantages and performance considerations in OpenMP tasking?</body>
            <answers>
                <answer>
                    <body>
                        Advantages: implicit load balancing, simple programming model, complications/bookkeeping pushed
                        to runtime.
                        <p/>
                        Considerations: Granularity and NUMA optimizaton.
                    </body>
                    <marks>
                        <mark type="keyword">balanc</mark>
                        <mark type="keyword">model</mark>
                        <mark type="keyword">runtime</mark>
                        <mark type="keyword">granular</mark>
                        <mark type="keyword">NUMA</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="53">
            <body>What is the difference between memory/cache coherency and memory consistency?</body>
            <answers>
                <answer>
                    <body>
                        Coherency: reason about updates to one memory location.
                        <p/>
                        Consistency: reason about updates to several memory locations.
                        <p/>
                    </body>
                    <marks>
                        <mark type="keyword">location</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="54">
            <body>When is a system coherent?</body>
            <answers>
                <answer>
                    <body>If program order for loads/stores is preserved, all stores eventually become visible, all
                        processors see same order of writes.
                    </body>
                    <marks>
                        <mark type="keyword">order</mark>
                        <mark type="keyword">visib</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="60054">
            <body>What ways are there to establish coherency?</body>
            <answers>
                <answer>
                    <body>Using snoop-based or directory based protocols.</body>
                    <marks>
                        <mark type="keyword">snoop</mark>
                        <mark type="keyword">directory</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="55">
            <body>What is sequential consistency?</body>
            <answers>
                <answer>
                    <body>[Lamport] “A multiprocessor system is sequentially consistent if the result of any execution
                        is the same as if the operations of all processors were executed in some sequential order, and
                        the operations of each individual processor appear in this sequence in the order specified by
                        the program”
                        <p/>
                        In other words, each processor issues memory instructions in program order that can be
                        sequentialized in any way that does not violate a processor's program order.
                    </body>
                    <marks>
                        <mark type="keyword">program</mark>
                        <mark type="keyword">order</mark>
                        <mark type="keyword">processor</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="56">
            <body>Why is sequential consistency hard?</body>
            <answers>
                <answer>
                    <body>Reordering elements such as compiler optimizations, instruction reordering, network elements,
                        etc. Write has to be atomic with respect to all processors (all need to see effect immediately).
                    </body>
                    <marks>
                        <mark type="keyword">order</mark>
                        <mark type="keyword">atom</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="57">
            <body>What is processor consistency and how is it weaker than sequential consistency?</body>
            <answers>
                <answer>
                    <body>Writes by any thread seen by all threads in order they were issued. But: Different threads may
                        see different orders.
                    </body>
                    <marks>
                        <mark type="keyword">issue</mark>
                        <mark type="keyword">order</mark>
                        <mark type="keyword">thread</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="58">
            <body>What is weak consistency and how is it weaker than sequential consistency?</body>
            <answers>
                <answer>
                    <body>In weak consistency, there are data operations and synchronization operations. Only
                        synchronization operations are sequentially consistent (flushing the memory pipeline).
                    </body>
                    <marks>
                        <mark type="keyword">data</mark>
                        <mark type="keyword">synchronization</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="59">
            <body>What is release consistency and how is it weaker than sequential consistency?</body>
            <answers>
                <answer>
                    <body>Subdivides synchronization operations into acquire and release (corresponding to lock
                        operations). Only acquires/releases have to be sequentially consistent.
                    </body>
                    <marks>
                        <mark type="keyword">lock</mark>
                        <mark type="keyword">sequen</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="60058">
            <body>What consistency model does OpenMP loosely conform to, what does that mean?</body>
            <answers>
                <answer>
                    <body>Weak consistency. Only synchronization operations are sequentially consistent.</body>
                    <marks>
                        <mark type="keyword">weak</mark>
                        <mark type="keyword">sync</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="60059">
            <body>
                Which of the following are not memory synchronization points (flush points)?
                <p/>
                <pre>
                    •Entry and exit of critical regions
                    •Barriers (implicit and explicit)
                    •Entry and exit of master regions
                    •Entry and exit of work sharing regions
                    •Every task scheduling point
                    •Use of OpenMP runtime locks
                    •Entry and exit of parallel regions
                </pre>
            </body>
            <answers>
                <answer>
                    <body>
                        <pre>
                            •Entry and exit of work sharing regions
                            •Entry and exit of master regions
                        </pre>
                    </body>
                    <marks>
                        <mark type="keyword">shar</mark>
                        <mark type="keyword">master</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="60">
            <body>
                What are potential problems with the following code? What construct is used to solve this?
                <p/>
                <pre>
                    //Thread 1
                    a = foo();
                    flag = 1;

                    //Thread 2
                    while (flag);
                    b = a;
                </pre>
            </body>
            <answers>
                <answer>
                    <body>Instruction reordering by compiler (doesn't see dependency). Variables might be kept in
                        registers. Need the flush directive to ensure that changes are visible and consistent.
                        <p/>
                        <pre>
                            <!-- @formatter:off -->
                            //Thread 1
                                a = foo();
                                #pragma omp flush(a, flag)
                                flag = 1;
                                #pragma omp flush(flag)

                            //Thread 2
                                while (flag)
                                {
                                    #pragma omp flush(flag)
                                }
                                #pragma omp flush(a, flag)
                                b = a;
                            <!-- @formatter:on -->
                        </pre>
                    </body>
                    <marks>
                        <mark type="keyword">order</mark>
                        <mark type="keyword">register</mark>
                        <mark type="keyword">flush</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="61">
            <body>When is there a control dependency between S1 and S2?</body>
            <answers>
                <answer>
                    <body>
                        Iff:
                        <p/>
                        There exists an execution path from S1 to S2
                        <p/>
                        Not all execution paths lead from S1 to S2
                    </body>
                    <marks>
                        <mark type="keyword">path</mark>
                        <mark type="keyword">exist</mark>
                        <mark type="keyword">all</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="62">
            <body>When is there a data dependency between S1 and S2?</body>
            <answers>
                <answer>
                    <body>
                        Iff:
                        <p/>
                        There exists an execution path from S1 to S2
                        <p/>
                        S1 and S2 access the same data, at least one is a write.
                    </body>
                    <marks>
                        <mark type="keyword">path</mark>
                        <mark type="keyword">data</mark>
                        <mark type="keyword">write</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="63">
            <body>What does O(S_1) and I(S_2) denote?</body>
            <answers>
                <answer>
                    <body>O(S_1) is the set of memory locations written by S_1. I(S_2) is the set of memory location
                        read by S_2.
                    </body>
                    <marks>
                        <mark type="keyword">writ</mark>
                        <mark type="keyword">read</mark>
                        <mark type="keyword">set</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="64">
            <body>What is the definition of a true dependence and what is the notation?</body>
            <answers>
                <answer>
                    <body>True dependence, iff O(S1) \cap I(S2) \neq \emptyset. Notation: (S1 \delta^t S2)</body>
                    <marks>
                        <mark type="keyword">O</mark>
                        <mark type="keyword">I</mark>
                        <mark type="keyword">delta</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="65">
            <body>What is the definition of an antidependence and what is the notation?</body>
            <answers>
                <answer>
                    <body>Antidependence, iff I(S1) \cap O(S2) \neq \emptyset. (S1 \delta^-1 S2).</body>
                    <marks>
                        <mark type="keyword">O</mark>
                        <mark type="keyword">I</mark>
                        <mark type="keyword">delta</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="66">
            <body>What is the definition of an output dependence and what is the notation?</body>
            <answers>
                <answer>
                    <body>Output dependence: O(S1) \cap O(S2) \neq \emptyset. Notation: (S1 \delta^o S2)</body>
                    <marks>
                        <mark type="keyword">O</mark>
                        <mark type="keyword">delta</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="67">
            <body>What types of data dependencies are there and what do they mean?</body>
            <answers>
                <answer>
                    <body>True dependency (RAW), anti dependency (WAR), output dependency (WAW)</body>
                    <marks>
                        <mark type="keyword">true</mark>
                        <mark type="keyword">anti</mark>
                        <mark type="keyword">output</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="68">
            <body>What is the meaning of loop independent dependencies?</body>
            <answers>
                <answer>
                    <body>All dependencies are within an iteration, none are across iterations.</body>
                    <marks>
                        <mark type="keyword">iteration</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="69">
            <body>What are loop carried dependencies? What is the problem?</body>
            <answers>
                <answer>
                    <body>A dependency across iterations, where the computation in one iteration depends on data written
                        in another.
                        <p/>
                        Iterations cannot be easily/directly reordered or executed in parallel.
                    </body>
                    <marks>
                        <mark type="keyword">iteration</mark>
                        <mark type="keyword">reorder</mark>
                        <mark type="keyword">parallel</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="70">
            <body>What is the problem with aliasing?</body>
            <answers>
                <answer>
                    <body>Instructions that might not seem like they have dependencies if the variables are at least
                        partially aliased, preventing potential optimizations.
                    </body>
                    <marks>
                        <mark type="keyword">dependenc</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="71">
            <body>What is needed to guarantee program correctness (ensure validity) if reordering happens?</body>
            <answers>
                <answer>
                    <body>The partial order of all dependencies must not change.</body>
                    <marks>
                        <mark type="keyword">dependenc</mark>
                        <mark type="keyword">order</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="72">
            <body>When is a dependence preserved?</body>
            <answers>
                <answer>
                    <body>iff the relative execution order of the dependencies' source and sink does not change.</body>
                    <marks>
                        <mark type="keyword">order</mark>
                        <mark type="keyword">source</mark>
                        <mark type="keyword">sink</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="73">
            <body>What is the nesting level of a loop?</body>
            <answers>
                <answer>
                    <body>The number of surrounding loops +1</body>
                    <marks>
                        <mark type="keyword">surround</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="74">
            <body>What is the iteration number?</body>
            <answers>
                <answer>
                    <body>In a normalized loop (0 to n-1), the loop number is equal to the value of the iterator.</body>
                    <marks>
                        <mark type="keyword">normal</mark>
                        <mark type="keyword">iterat</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="75">
            <body>What is the iteration vector?</body>
            <answers>
                <answer>
                    <body>A tuple containing all iteration numbers, outer to inner.</body>
                    <marks>
                        <mark type="keyword">number</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="76">
            <body>What is an iteration space?</body>
            <answers>
                <answer>
                    <body>The set of all possible iteration vectors for a statement is an iteration space.</body>
                    <marks>
                        <mark type="keyword">set</mark>
                        <mark type="keyword">vector</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="77">
            <body>When does an iteration i precede an iteration j?</body>
            <answers>
                <answer>
                    <body>Iff, \exists k, i[r] = j[r], \forall r: 1&gt;=r&gt;=k and i[k]&gt;j[k]</body>
                    <marks>
                        <mark type="keyword">k</mark>
                        <mark type="keyword">r</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="78">
            <body>What is a statement instance and what is the notation?</body>
            <answers>
                <answer>
                    <body>For an iteration vector i, a statement instance refers to a statement execution in that
                        particular iteration vector.
                    </body>
                    <marks>
                        <mark type="keyword">vector</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="79">
            <body>What is the dependence distance for (S1(\vec{i}) \delta S2(\vec{j}))?</body>
            <answers>
                <answer>
                    <body>\vec{j}-\vec{i}</body>
                    <marks>
                        <mark type="keyword">j</mark>
                        <mark type="keyword">i</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="80">
            <body>How is the dependence direction calculated?</body>
            <answers>
                <answer>
                    <body>"&lt;", "=", ">", according to sign of the loop dependence distance.
                        <p/>
                        d(i,j)_k > 0 -> "&lt;",
                        d(i,j)_k = 0 -> "=",
                        d(i,j)_k &lt; 0 -> ">"
                    </body>
                    <marks>
                        <mark type="keyword">sign</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="60079">
            <body>What is the level of a loop carried dependency?</body>
            <answers>
                <answer>
                    <body>The index of the left most non-"=" direction vector entry.</body>
                    <marks>
                        <mark type="keyword">=</mark>
                        <mark type="keyword">vector</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="60080">
            <body>What are available transformations?</body>
            <answers>
                <answer>
                    <body>Loop interchange, loop fission/fusion, loop alignment.</body>
                    <marks>
                        <mark type="keyword">interchange</mark>
                        <mark type="keyword">fusion</mark>
                        <mark type="keyword">fission</mark>
                        <mark type="keyword">alignment</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="81">
            <body>When is a loop interchange safe?</body>
            <answers>
                <answer>
                    <body>If the direction vectors of all dependencies has only "=", or the direction vectors of all
                        dependence vectors do not has ">" as the leftmost non-"-" direction.
                    </body>
                    <marks>
                        <mark type="keyword">vector</mark>
                        <mark type="keyword">=</mark>
                        <mark type="keyword">></mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="82">
            <body>When is loop distribution safe?</body>
            <answers>
                <answer>
                    <body>For 2 sets of statements, if no dependency cycle exists between those groups (outer loop
                        carried dependencies can be ignored). The order of new loops needs to preserve dependencies
                        between the sets.
                    </body>
                    <marks>
                        <mark type="keyword">cycle</mark>
                        <mark type="keyword">outer</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="83">
            <body>What can loop fusion introduce?</body>
            <answers>
                <answer>
                    <body>Loop carried dependencies.</body>
                    <marks>
                        <mark type="keyword">carr</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="84">
            <body>What effect does loop fusion/fission have on granularity?</body>
            <answers>
                <answer>
                    <body>Fusion: reduces granularity. Fission: increases granularity.</body>
                    <marks>
                        <mark type="keyword">granularity</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="85">
            <body>What is loop alignment useful for?</body>
            <answers>
                <answer>
                    <body>Changing a carried dependence into an independent dependence.</body>
                    <marks>
                        <mark type="keyword">carr</mark>
                        <mark type="keyword">independen</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="86">
            <body>What types of races can exist?</body>
            <answers>
                <answer>
                    <body>The same ones as in dependencies. RAW, WAR, WAW.</body>
                    <marks>
                        <mark type="keyword">dependenc</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="87">
            <body>What do race detection tools do?</body>
            <answers>
                <answer>
                    <body>Static/dynamic instrumentation of all memory accesses and tracking of synchronization to
                        detect unsynchronized accesses to the same memory.
                    </body>
                    <marks>
                        <mark type="keyword">instrument</mark>
                        <mark type="keyword">track</mark>
                        <mark type="keyword">memory</mark>
                        <mark type="keyword">sync</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="88">
            <body>What are the 3 performance aspects in parallel code?</body>
            <answers>
                <answer>
                    <body>Synchronization overhead, cache behavior and locality, thread and data locality mapping.
                    </body>
                    <marks>
                        <mark type="keyword">sync</mark>
                        <mark type="keyword">cache</mark>
                        <mark type="keyword">local</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="89">
            <body>What is the ABA Problem?</body>
            <answers>
                <answer>
                    <body>
                        For example in list removal:
                        <p/>
                        A tries to remove an element but has to wait
                        <p/>
                        B removes the element, frees it, reallocates it, inserts element again
                        <p/>
                        A compares against the new element but thinks it's still the old one (using pointers)
                        <p/>
                    </body>
                    <marks>
                        <mark type="keyword">remove</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="90">
            <body>When does the ABA problem occur?</body>
            <answers>
                <answer>
                    <body>If pointers are compared and reallocation occurs. Can be combated by generational counters.
                    </body>
                    <marks>
                        <mark type="keyword">pointer</mark>
                        <mark type="keyword">realloc</mark>
                        <mark type="keyword">generation</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="91">
            <body>What is the difference between a lock free data structure and a wait-free data structure?</body>
            <answers>
                <answer>
                    <body>
                        Lock free: avoids call to mutexes, but can be hard to get right and needs hardware support.
                        <p/>
                        Wait free: every operation finishes in a finite number of steps, which is much harder to
                        guarantee.
                    </body>
                    <marks>
                        <mark type="keyword">mutex</mark>
                        <mark type="keyword">finite</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="92">
            <body>What does libnuma offer?</body>
            <answers>
                <answer>
                    <body>
                        Memory allocation using
                        <pre>
                            numa_alloc_onnode
                            numa_alloc_local
                        </pre>
                        and thread binding
                        <pre>
                            numa_bind
                        </pre>
                    </body>
                    <marks>
                        <mark type="keyword">alloc</mark>
                        <mark type="keyword">bind</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="93">
            <body>What does the operation vfmadd213pd do?</body>
            <answers>
                <answer>
                    <body>Fused multiply and add, where vfmadd213pd(a, b, c) results in dest[i]=a[i]*b[i]+c[i].</body>
                    <marks>
                        <mark type="keyword">mult</mark>
                        <mark type="keyword">add</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="94">
            <body>What does SIMD mask do?</body>
            <answers>
                <answer>
                    <body>Masks influence which destination values of the SIMD operation are affected.</body>
                    <marks>
                        <mark type="keyword">dest</mark>
                        <mark type="keyword">valu</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="95">
            <body>What is an alternative to writing SIMD intrinsics code?</body>
            <answers>
                <answer>
                    <body>Using compiler auto-vectorization, and inspecting the optimization report.</body>
                    <marks>
                        <mark type="keyword">vector</mark>
                        <mark type="keyword">report</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="96">
            <body>How can a loop with a loop carried dependency be vectorized?</body>
            <answers>
                <answer>
                    <body>Freely, as long as the data width stays smaller than the smallest loop carried dependency
                        distance.
                    </body>
                    <marks>
                        <mark type="keyword">distance</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="97">
            <body>What are some reasons that auto-vectorization could fail?</body>
            <answers>
                <answer>
                    <body>Alignment, function calls, complex control flow, loop not "countable" (unpredictable while
                        loop etc).
                    </body>
                    <marks>
                        <mark type="keyword">align</mark>
                        <mark type="keyword">function</mark>
                        <mark type="keyword">control</mark>
                        <mark type="keyword">count</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="98">
            <body>What does the OpenMP SIMD construct do?</body>
            <answers>
                <answer>
                    <body>Cuts the loop into chunks that fit the target SIMD architecture.</body>
                    <marks>
                        <mark type="keyword">chunk</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="99">
            <body>What do the OpenMP clauses safelen, linear, aligned and collapse do?</body>
            <answers>
                <answer>
                    <body>
                        Safelen: Maximum number of concurrent iterations that do not break dependence.
                        <p/>
                        Linear: Variable's value has linear relationship with iteration number.
                        <p/>
                        Aligned: specifies variable alignment
                        <p/>
                        Collapse: TODO
                    </body>
                    <marks></marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="100">
            <body>What does the OpenMP clause schedule(simd: static, 5) do?</body>
            <answers>
                <answer>
                    <body>Schedules chunk sizes that are multiples of the SIMD length, changing first/last iteration as
                        necessary to fix alignment and extra iterations. No remaineder loops are triggered.
                    </body>
                    <marks>
                        <mark type="keyword">length</mark>
                        <mark type="keyword">align</mark>
                        <mark type="keyword">remainder</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="101">
            <body>How can you include a function in a SIMD parallel loop?</body>
            <answers>
                <answer>
                    <body>Using <pre>#pragma omp declare simd</pre> on the function declaration
                    </body>
                    <marks>
                        <mark type="keyword">declare</mark>
                        <mark type="keyword">simd</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="102">
            <body>What are common data layouts to be used with SIMD and what are some pros/cons?</body>
            <answers>
                <answer>
                    <body>Array of structs (good locality of members, 1 memory stream). Struct of arrays (contiguous
                        load/store, but poor locality of members, 3 memory streams necessary). Hybrid (AoSoA).
                    </body>
                    <marks>
                        <mark type="keyword">array</mark>
                        <mark type="keyword">struct</mark>
                        <mark type="keyword">locality</mark>
                        <mark type="keyword">stream</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="103">
            <body>How can data alignment be ensured for multi-dimensional array?</body>
            <answers>
                <answer>
                    <body>Using padding between rows, etc.</body>
                    <marks>
                        <mark type="keyword">pad</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="104">
            <body>What is a problem with ethernet for high performance computing? What is a possible solution?</body>
            <answers>
                <answer>
                    <body>
                        Sending messages goes through the kernel, across a tcp/ip stack into an ethernet driver and
                        onto the hardware.
                        <p/>
                        User level communication does only the setup through the kernel and all subsequent communication
                        is possible directly in user space, enabling higher bandwidth and low latency.
                    </body>
                    <marks>
                        <mark type="keyword">kernel</mark>
                        <mark type="keyword">setup</mark>
                        <mark type="keyword">user</mark>
                        <mark type="keyword">bandwidth</mark>
                        <mark type="keyword">latency</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="105">
            <body>What is VIA and what are common examples?</body>
            <answers>
                <answer>
                    <body>Virtual Interface Architecture, Infiniband (offloading), Omnipath (onloading)</body>
                    <marks>
                        <mark type="keyword">virtual</mark>
                        <mark type="keyword">infini</mark>
                        <mark type="keyword">omni</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="100105">
            <body>How does an application communicate through VIA once setup is complete?</body>
            <answers>
                <answer>
                    <body>The data is written/read by the VI User Agent into/from message queues which interface
                        directly with hardware.
                    </body>
                    <marks>
                        <mark type="keyword">queue</mark>
                        <mark type="keyword">agent</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="100106">
            <body>What are some common network topologies?</body>
            <answers>
                <answer>
                    <body>Fat-Tree, Torus, Dragonfly (intra-connect: full-mesh, inter-connect: partitioned to all),
                        Hypercube
                    </body>
                    <marks>
                        <mark type="keyword">tree</mark>
                        <mark type="keyword">torus</mark>
                        <mark type="keyword">dragonfly</mark>
                        <mark type="keyword">hypercube</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="100107">
            <body>What other nodes except for compute nodes does a HPC system usually have?</body>
            <answers>
                <answer>
                    <body>Storage (archive) nodes, login/head/compilation nodes, system nodes (reliability,
                        availability, servicability)
                    </body>
                    <marks>
                        <mark type="keyword">storage</mark>
                        <mark type="keyword">login</mark>
                        <mark type="keyword">system</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="106">
            <body>What is MPI?</body>
            <answers>
                <answer>
                    <body>MPI is an Application Programming Interface specification (no ABI, recompile to change
                        implementation), implemented as a library.
                    </body>
                    <marks>
                        <mark type="keyword">application programming interface</mark>
                        <mark type="keyword">specification</mark>
                        <mark type="keyword">library</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="107">
            <body>What conditions have to hold with MPI_Finalize?</body>
            <answers>
                <answer>
                    <body>All communication must be completed, needs to be called by all MPI processes and is
                        collective. Must only be called after MPI_Init.
                    </body>
                    <marks>
                        <mark type="keyword">communicat</mark>
                        <mark type="keyword">process</mark>
                        <mark type="keyword">collective</mark>
                        <mark type="keyword">init</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="108">
            <body>How does addressing work in MPI?</body>
            <answers>
                <answer>
                    <body>A process is uniquely identified by a communicator and its rank within that communicator
                    </body>
                    <marks>
                        <mark type="keyword">communicat</mark>
                        <mark type="keyword">rank</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="109">
            <body>What data is part of the MPI_Status structure?</body>
            <answers>
                <answer>
                    <body>The actual MPI_SOURCE, MPI_TAG, MPI_ERROR</body>
                    <marks>
                        <mark type="keyword">source</mark>
                        <mark type="keyword">tag</mark>
                        <mark type="keyword">error</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="110">
            <body>How can you get the size of the actually received data in MPI?</body>
            <answers>
                <answer>
                    <body>Using MPI_Get_Count(*status, dtype, *count)</body>
                    <marks>
                        <mark type="keyword">get_count</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="111">
            <body>What do MPI_Wtime and MPI_Wtick do?</body>
            <answers>
                <answer>
                    <body>Wtime: wall time in seconds (not necessarily synchronized). Wtick: resolution of wall time.
                    </body>
                    <marks>
                        <mark type="keyword">wall</mark>
                        <mark type="keyword">resolution</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="112">
            <body>Why is central initialization generally avoided in a MPI application?</body>
            <answers>
                <answer>
                    <body>One node alone might not have enough resources to initialize the data structure and lots of
                        communication needs to happen to distribute it.
                    </body>
                    <marks>
                        <mark type="keyword">resource</mark>
                        <mark type="keyword">communicat</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="113">
            <body>What is the logical topology in a MPI Application?</body>
            <answers>
                <answer>
                    <body>The mapping of local pieces into the global data structure. Usually important to know, where
                        the neighbors are.
                    </body>
                    <marks>
                        <mark type="keyword">local</mark>
                        <mark type="keyword">global</mark>
                        <mark type="keyword">neighbor</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="114">
            <body>Which components can an message traverse between the send and receive?</body>
            <answers>
                <answer>
                    <body>Send -> (Send Buffer, optional) -> (Receive Queue, direct delivery)/Unexpected Message Queue
                        -> Receive
                    </body>
                    <marks>
                        <mark type="keyword">buffer</mark>
                        <mark type="keyword">queue</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="115">
            <body>What do the Isend, Bsend, Rsend, Ssend variants of MPI calls do?</body>
            <answers>
                <answer>
                    <body>I: Non-blocking, B: buffered, R: ready (application guarantees receiver ready), S: synchronize
                        (returns once receive started).
                    </body>
                    <marks>
                        <mark type="keyword">block</mark>
                        <mark type="keyword">buffer</mark>
                        <mark type="keyword">ready</mark>
                        <mark type="keyword">synchroniz</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="116">
            <body>What does MPI_Probe/MPI_Iprobe do?</body>
            <answers>
                <answer>
                    <body>Waits for an incoming message without actually receiving it, outputs status object (containing
                        specific source and tag, useful for wildcard).
                    </body>
                    <marks>
                        <mark type="keyword">message</mark>
                        <mark type="keyword">status</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="117">
            <body>What is the eager limit?</body>
            <answers>
                <answer>
                    <body>The maximum message size for which the eager protocol is still used instead of a rendezvous.
                    </body>
                    <marks>
                        <mark type="keyword">rendezvous</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="118">
            <body>What does MPI_Sendrecv do? (Write code).</body>
            <answers>
                <answer>
                    <body>
                        Does a blocking simultaneous send and receive operations, to avoid deadlocks.
                        <pre>
                            <!-- @formatter:off -->
                            int out = -1; int in = 0;
                            MPI_Sendrecv(&amp;out, 1 MPI_INT, DEST, TAG, &amp;in, 1, MPI_INT, SOURCE, TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
                            <!-- @formatter:on -->
                        </pre>
                    </body>
                    <marks>
                        <mark type="keyword">deadlock</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="119">
            <body>What does MPI_Sendrec_replace do? (Write code).</body>
            <answers>
                <answer>
                    <body>
                        Does a blocking simultaneous send and receive operations, to avoid deadlocks, additionally
                        reusing send buffer for receive.
                        <pre>
                            <!-- @formatter:off -->
                            int inout = -1;
                            MPI_Sendrecv_replace(&amp;inout, 1 MPI_INT, DEST, TAG, SOURCE, TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
                            <!-- @formatter:on -->
                        </pre>
                    </body>
                    <marks>
                        <mark type="keyword">buffer</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="120">
            <body>What does MPI_Test do? (Write code)</body>
            <answers>
                <answer>
                    <body>Checks for completion of MPI non-blocking operation:
                        <pre>
                            <!-- @formatter:off -->
                            int flag = -1;
                            MPI_Test(&amp;request, &amp;flag, MPI_STATUS_IGNORE);
                            <!-- @formatter:on -->
                        </pre>
                        True, if request completed or request is MPI_REQUEST_NULL. If completed, set to
                        MPI_REQUEST_NULL.
                    </body>
                    <marks>
                        <mark type="keyword">complet</mark>
                        <mark type="keyword">null</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="121">
            <body>What are the three classes of collective operations in MPI?</body>
            <answers>
                <answer>
                    <body>Synchronization (Barrier and flavors), Communication (Broadcast, Gather, Scatter, ...),
                        Reduction (...)
                    </body>
                    <marks>
                        <mark type="keyword">sync</mark>
                        <mark type="keyword">communicat</mark>
                        <mark type="keyword">reduce</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="122">
            <body>How can a value from one process be copied to all processes in MPI?</body>
            <answers>
                <answer>
                    <body>
                        Using MPI_Bcast:
                        <pre>
                            int MPI_Bcast(void *buf, int count, MPI_Datatype dtype, int root, MPI_Comm comm)
                        </pre>
                    </body>
                    <marks>
                        <mark type="keyword">bcast</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="123">
            <body>In what order will the values appear if they are gathered with root 0? P0: {A0,A1} P1: {B0,B1}, P2:
                {C0,C1}
            </body>
            <answers>
                <answer>
                    <body>P0: (A0, A1, B0, B1, C0, C1)</body>
                    <marks>
                        <mark type="keyword">A0</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="124">
            <body>Write code that collects information of different sizes from all processes to the process with rank
                0.
            </body>
            <answers>
                <answer>
                    <body>Using
                        <pre>
                            MPI_Gatherv(void *sendbuf, int sendcount, MPI_Datatype sendtype, void* recvbuf,
                            int*recvcount, int* displs, MPI_Datatype recvtype, int root, MPI_Comm comm);
                        </pre>
                        where displs has the first index in the receive buffer (array?) for each process.
                    </body>
                    <marks>
                        <mark type="keyword">gatherv</mark>
                        <mark type="keyword">disp</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="125">
            <body>What does MPI_Alltoall do and what is the problem?</body>
            <answers>
                <answer>
                    <body>Sends data from each process to each other process, which scales badly O(n^2)</body>
                    <marks>
                        <mark type="keyword">process</mark>
                        <mark type="keyword">scale</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="126">
            <body>How does a reduction work in MPI?</body>
            <answers>
                <answer>
                    <body>
                        <pre>
                            int MPI_Reduce(void* sbuf, void* rbuf, int count, MPI_Datatype dtype, MPI_Op op, int root,
                            MPI_Comm comm)
                        </pre>
                        Available Ops: MPI_SUM, ..., User defined, ...
                        Often implemented as tree structure.
                    </body>
                    <marks>
                        <mark type="keyword">op</mark>
                        <mark type="keyword">mpi_reduce</mark>
                        <mark type="keyword">tree</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="127">
            <body>What is an Ibarrier useful for?</body>
            <answers>
                <answer>
                    <body>To continue doing useful work while other processes catch up to the synchronization.</body>
                    <marks>
                        <mark type="keyword">work</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="128">
            <body>What is persistent communication in MPI?</body>
            <answers>
                <answer>
                    <body>Operations that are initialized once ahead of time using MPI_*Op*_init, and then started using
                        MPI_Start, allowing for further optimization.
                    </body>
                    <marks>
                        <mark type="keyword">init</mark>
                        <mark type="keyword">start</mark>
                        <mark type="keyword">optim</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="129">
            <body>Why should libraries have their own communicators and how is this easily achieved?</body>
            <answers>
                <answer>
                    <body>To avoid cross-talk (especially with wildcards). Could use MPI_Comm_dup(old, new).</body>
                    <marks>
                        <mark type="keyword">cross</mark>
                        <mark type="keyword">dup</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="130">
            <body>What does MPI_Type_contiguous do?</body>
            <answers>
                <answer>
                    <body>Declares an MPI type that is multiple contiguous occurrences of a data type in memory:
                        <pre>
                            MPI_Datatype newtype;
                            int MPI_Type_contiguous(int count, MPI_Datatype oldtype, MPI_Datatype* newtype)
                        </pre>
                    </body>
                    <marks>
                        <mark type="keyword">multiple</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="131">
            <body>How does MPI_Type_vector work and what are the relevant measurements?</body>
            <answers>
                <answer>
                    <body>
                        <pre>
                            int MPI_Type_vector(int count, int blocklength, int stride, MPI_Datatype oldtype,
                            MPI_Datatype *newtype);
                        </pre>
                        Creates blocks of oldtypes, which start at distance stride.
                        Stride: Distance from beginning of one occurrence of a set of oldtypes to beginning of next
                        occurrence.
                        Count: The number of blocks/strides.
                        Blocklength: The size of the of a set of oldtypes.
                        Extent: The distance from beginning of first set to the end of the last.
                        <img src="../data/res/pp/131_vector.png"/>
                    </body>
                    <marks>
                        <mark type="keyword">stride</mark>
                        <mark type="keyword">count</mark>
                        <mark type="keyword">blocklength</mark>
                        <mark type="keyword">extent</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <!-- TODO: Indexed, struct, subarray types -->
        <question type="extended-answer" id="132">
            <body>What MPI routines are available for inspecting data types?</body>
            <answers>
                <answer>
                    <body>MPI_Type_get_envelope, returns counts of elements used in construction and a combiner (int:
                        method used to construct data type). MPI_Type_get_contents, gets the actual information based on
                        the counts (delimited by max values).
                    </body>
                    <marks>
                        <mark type="keyword">envelope</mark>
                        <mark type="keyword">contents</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="133">
            <body>What are MPI_Datatypes useful for?</body>
            <answers>
                <answer>
                    <body>Messages that combine multiple types, access to "spread out" data structures. Allows for
                        optimization, but incurs overhead.
                    </body>
                    <marks>
                        <mark type="keyword">types</mark>
                        <mark type="keyword">spread</mark>
                        <mark type="keyword">optimiz</mark>
                        <mark type="keyword">overhead</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="134">
            <body>What are advantages of two-sided/one-sided communication?</body>
            <answers>
                <answer>
                    <body>Two-sided: Simple concept, clear communication locations, implicit synchronization, no extra
                        synchronization required. One-sided: Receiver not involved, sender not delayed,
                    </body>
                    <marks>
                        <mark type="keyword">locat</mark>
                        <mark type="keyword">sync</mark>
                        <mark type="keyword">receiv</mark>
                        <mark type="keyword">delay</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="135">
            <body>What is the idea behind one-sided communication?</body>
            <answers>
                <answer>
                    <body>Provide ability to move data without requiring remote process to synchronize (decouple data
                        movement with process synchronization).
                    </body>
                    <marks>
                        <mark type="keyword">sync</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="136">
            <body>What are the necessary steps to use RMA in MPI?</body>
            <answers>
                <answer>
                    <body>Make memory accessible, read/write/update remote memory, add synchronization as needed.</body>
                    <marks>
                        <mark type="keyword">access</mark>
                        <mark type="keyword">memory</mark>
                        <mark type="keyword">sync</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="137">
            <body>What is a Window in MPI?</body>
            <answers>
                <answer>
                    <body>An abstraction of a limited memory region, tied to a communicator using the window context.
                    </body>
                    <marks>
                        <mark type="keyword">memory</mark>
                        <mark type="keyword">region</mark>
                        <mark type="keyword">communicat</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="138">
            <body>What options exist to make local memory collectively accessible?</body>
            <answers>
                <answer>
                    <body>
                        MPI_Win_create for existing memory:
                        <pre>
                            int MPI_Win_create(void *base, MPI_Aint size, int disp_unit, MPI_Info info, MPI_Comm comm,
                            MPI_Win* win);
                        </pre>
                        MPI_Win_allocate for memory yet to be allocated:
                        <pre>
                            int MPI_Win_allocate(MPI_Aint size, int disp_unit, MPI_Info info, MPI_Comm comm, void *base,
                            MPI_Win* win);
                        </pre>
                        MPI_Win_create_dynamic for when no buffer is currently available. Can later be attached using
                        MPI_Win_attach.
                        <pre>
                            int MPI_Win_create_dynamic(MPI_Info info, MPI_Comm comm, MPI_Win *win);
                            int MPI_Win_attach(MPI_Win win, void *base, MPI_Aint size);
                            int MPI_Win_detach(MPI_Win win, void* base);
                        </pre>
                    </body>
                    <marks>
                        <mark type="keyword">create</mark>
                        <mark type="keyword">allocate</mark>
                        <mark type="keyword">dynamic</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="139">
            <body>What does MPI_Get do and how is it called?</body>
            <answers>
                <answer>
                    <body>
                        <pre>
                            int MPI_Get(void *origin_addr, int origin_count, MPI_Datatype origin_datatype,
                            int target_rank, MPI_Aint target_disp, int target_count, MPI_Datatype target_datatype,
                            MPI_Win win)
                        </pre>
                        Fetches data from a target (RMA window) into the origin.
                    </body>
                    <marks>
                        <mark type="keyword">target</mark>
                        <mark type="keyword">origin</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="140">
            <body>What does MPI_Put do and how is it called?</body>
            <answers>
                <answer>
                    <body>
                        <pre>
                            int MPI_Put(void* origin_addr, int origin_count, MPI_Datatype origin_datatype,
                            int target_rank, MPI_Aint target_disp, int target_count, MPI_Datatype target_datatype,
                            MPI_Win win)
                        </pre>
                        Stores data from an origin into the target (RMA window).
                    </body>
                    <marks>
                        <mark type="keyword">origin</mark>
                        <mark type="keyword">target</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="141">
            <body>Which operation does MPI_Accumulate resemble and what is the difference?</body>
            <answers>
                <answer>
                    <body>Like an MPI_Put, but additionally applies MPI_Op using value already there.</body>
                    <marks>
                        <mark type="keyword">put</mark>
                        <mark type="keyword">op</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="142">
            <body>Which operation does MPI_Replace resemble and what is the difference?</body>
            <answers>
                <answer>
                    <body>Like an MPI_Put, but does so in an atomic manner. Usually, two puts to the same location can
                        produce garbage.
                    </body>
                    <marks>
                        <mark type="keyword">put</mark>
                        <mark type="keyword">atomic</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="143">
            <body>What are the three synchronization models for MPI RMA?</body>
            <answers>
                <answer>
                    <body>Fence (on target), Post(Target)/Start(Origin)/Complete(Origin)/Wait(Target), Lock/Unlock
                        synchronization (passive target).
                    </body>
                    <marks>
                        <mark type="keyword">fence</mark>
                        <mark type="keyword">post</mark>
                        <mark type="keyword">lock</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="144">
            <body>What does lock/unlock synchronization in MPI RMA not do and what is special about operations within
                its epoch?
            </body>
            <answers>
                <answer>
                    <body>
                        Does not provide mutually exclusive access. All operations in lock/unlock synchronization are
                        nonblocking.
                    </body>
                    <marks>
                        <mark type="keyword">exclusiv</mark>
                        <mark type="keyword">nonblock</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="145">
            <body>How is shared memory used in MPI and what operations can be done on the shared memory?</body>
            <answers>
                <answer>
                    <body>
                        <pre>
                            int MPI_Win_allocate_shared(MPI_Aint size, int disp_unit, MPI_Info info, MPI_Comm comm,
                            void* baseptr, MPI_Win *win)
                        </pre>
                        Supports load/store operations
                    </body>
                    <marks>
                        <mark type="keyword">shared</mark>
                        <mark type="keyword">load</mark>
                        <mark type="keyword">store</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="146">
            <body>What are two approaches to combining shared memory and distributed memory, and why?</body>
            <answers>
                <answer>
                    <body>MPI only (single model, portability), hybrid (MPI+OpenMP, etc.)</body>
                    <marks>
                        <mark type="keyword">mpi</mark>
                        <mark type="keyword">openmp</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="147">
            <body>What levels of MPI threading are available and what call is used to ensure availability?</body>
            <answers>
                <answer>
                    <body>
                        <pre>
                            int MPI_Init_thread(int *argc, char ***argv, int required, int *provided)
                        </pre>
                        MPI_THREAD_SINGLE, MPI_THREAD_FUNNELED, MPI_THREAD_SERIALIZED, MPI_THREAD_MULTIPLE
                    </body>
                    <marks>
                        <mark type="keyword">init_thread</mark>
                        <mark type="keyword">single</mark>
                        <mark type="keyword">funnel</mark>
                        <mark type="keyword">serialize</mark>
                        <mark type="keyword">multiple</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="148">
            <body>What is the consequence of using MPI_THREAD_MULTIPLE?</body>
            <answers>
                <answer>
                    <body>Each call completes on its own, user responsible for ensuring that no racing collectives are
                        posted.
                    </body>
                    <marks>
                        <mark type="keyword">call</mark>
                        <mark type="keyword">rac</mark>
                        <mark type="keyword">collect</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="149">
            <body>What is the problem with MPI_Probe and threading?</body>
            <answers>
                <answer>
                    <body>If multiple threads probe and return, they may not receive the message they posted. Use
                        Mprobe/Mrecv to match probe with receive call.
                    </body>
                    <marks>
                        <mark type="keyword">message</mark>
                        <mark type="keyword">mprobe</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <!-- TODO: Indexed, struct, subarray types -->
        <!-- TODO: Continue Chapter 10 Slide 0 -->

        <!-- From Exam 2018 -->
        <question type="extended-answer" id="1000">
            <body>1a) What can be computed by applying Amdahl's Law [and state the law]?</body>
            <answers>
                <answer>
                    <body>Using Amdahl's Law, we can compute the minimum achievable runtime in a program
                        consisting of sequential and parallel regions for a given number of processors.
                        Amdahl's Law states that <pre>T(p)= (1-f)T + f(T/p)</pre>, where f is fraction of parallel
                        execution and p number of processes. From this, it is possible to derive the maximum speedup
                        S(p)=1/(1-f+f/p).
                    </body>
                    <marks>
                        <mark type="keyword">speedup</mark>
                        <mark type="keyword">sequen</mark>
                        <mark type="keyword">par</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10001">
            <body>1b) Assume you have a program, which is split into regions that either cannot be parallelized at all
                or that can be parallelized perfectly. Assuming the regions that cannot be parallelized consume 1% of
                execution time during a sequential run, what is the maximal speedup that can be achieved?
            </body>
            <answers>
                <answer>
                    <body>Using Amdahl's Law with f=0.99, the maximum achievable speedup is 1/(1-f+f/p), with p very
                        large. Thus 1/(1-0.99) = 1/0.01 = 100.
                    </body>
                    <marks>
                        <mark type="keyword">100</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10002">
            <body>2a) Name one possible reason for a negative performance impact in case lock granularity is chosen too
                fine.
            </body>
            <answers>
                <answer>
                    <body>When lock granularity is too fine there are many lock/unlock operations when accessing
                        resources, the processing of which detracts from the time spent on useful computation.
                    </body>
                    <marks>
                        <mark type="keyword">lock</mark>
                        <mark type="keyword">time</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10003">
            <body>2b) Name one possible reason for a negative performance impact in case lock granularity is chosen too
                coarse:
            </body>
            <answers>
                <answer>
                    <body>When lock granularity is too coarse, accessing independent resources (such as different
                        locations in a large array) can block just because they are protected by the same lock (for the
                        whole array). This requires a process to wait even though the resource could be accessed.
                    </body>
                    <marks>
                        <mark type="keyword">lock</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10004">
            <body>3a) Parallelize for loop in the following function using OpenMP by replacing &gt;OPENMP HERE&lt;with
                the appropriate OpenMP directive(s) and clause(s). Write your solution in the box below.
                <pre>
                    <!-- @formatter:off -->
                    1   void function(float* a,float* b,int n) {
                    2      int i;
                    3      &lt;OPENMP HERE&gt;
                    4      for(i = 0; i &lt; n - 1; i++)
                    5          a[i] = (b[i] + b[i + 1]) / 2;
                    6      a[i] = b[i] / 2;
                    7   }
                    <!-- @formatter:on -->
                </pre>
            </body>
            <answers>
                <answer>
                    <body>
                        <pre>
                            #pragma omp parallel for lastprivate(i) shared(a, b, n)
                        </pre>
                    </body>
                    <marks>
                        <mark type="keyword">private</mark>
                        <mark type="keyword">shared</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10005">
            <body>b)* Parallelize for loop in the following function using OpenMP by replacing &gt;OPENMP HERE&lt; with
                the appropriate OpenMP directive(s) and clause(s). Write your solution in the box below.
                <pre>
                    <!-- @formatter:off -->
                    double function(float* x,int* y,int n)
                    {
                        int i, b = y[0];
                        float a = 1.0;
                        &lt;OPENMP HERE>
                        for(i=0; i &gt; n; i++) {
                            a += x[i];
                            if(b &lt; y[i])
                                b = y[i];
                        }
                        return a * b;
                    }
                    <!-- @formatter:on -->
                </pre>
            </body>
            <answers>
                <answer>
                    <body>#pragma omp parallel for private(i) reduction(+: a) reduction(min: b) shared(x, y)</body>
                    <marks>
                        <mark type="keyword">reduction</mark>
                        <mark type="keyword">+</mark>
                        <mark type="keyword">min</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10006">
            <body>
                Consider the following program being executed with OMP_NUM_THREADS set to 16.
                <pre>
                    <!-- @formatter:off -->
                    int value[64], k, t;
                    for(inti=0; i &lt; 64; i++)
                        value[i]=63-i;
                    t=omp_get_thread_num();
                    k=42;
                    #pragma omp parallel for schedule(static, 2) CLAUSE
                    for(inti=0; i &lt; 64; i++) {
                        k=value[i]*2+t;
                    }
                    printf("Final value of k=%i\n", k);
                    <!-- @formatter:on -->
                </pre>
                a)* Which iterations are executed by thread 13 in the parallel for loop?
            </body>
            <answers>
                <answer>
                    <body>Iterations 26, 27 and 58, 59</body>
                    <marks>
                        <mark type="keyword">26</mark>
                        <mark type="keyword">27</mark>
                        <mark type="keyword">58</mark>
                        <mark type="keyword">59</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10007">
            <body>
                Consider the following program being executed with OMP_NUM_THREADS set to 16.
                <pre>
                    <!-- @formatter:off -->
                    int value[64], k, t;
                    for(inti=0; i &lt; 64; i++)
                        value[i]=63-i;
                    t=omp_get_thread_num();
                    k=42;
                    #pragma omp parallel for schedule(static, 2) CLAUSE
                    for(inti=0; i &lt;64; i++) {
                        k=value[i]*2+t;
                    }
                    printf("Final value of k=%i\n", k);
                    <!-- @formatter:on -->
                </pre>
                b)* Fill in the following table. If multiple values are possible, state MULTIPLE.
                <pre>
                    CLAUSE set to | Printed value of k
                    NONE |
                    private(k,t) |
                    shared(k,t) |
                    firstprivate(k,t) |
                    lastprivate(k,t) |
                    firstprivate(k,t) lastprivate(k,t) |
                </pre>
            </body>
            <answers>
                <answer>
                    <body>
                        <pre>
                            CLAUSE set to | Printed value of k
                            NONE | MULTIPLE (default shared)
                            private(k,t) | 42
                            shared(k,t) | MULTIPLE
                            firstprivate(k,t) | 42
                            lastprivate(k,t) | MULTIPLE (due to uninitialized, observed only 0)
                            firstprivate(k,t) lastprivate(k,t) | 0
                        </pre>
                    </body>
                    <marks>
                        <mark type="keyword">MULTIPLE</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10008">
            <body>
                a)* List the OpenMP pragma used to specify a task [and the clause to ensure a task completes]:
            </body>
            <answers>
                <answer>
                    <body>
                        <pre>
                            #pragma omp task
                            #pragma omp taskwait
                        </pre>
                    </body>
                    <marks>
                        <mark type="keyword">task</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10009">
            <body>b)* What is the difference between a tied and an untied task?</body>
            <answers>
                <answer>
                    <body>A tied task is guaranteed to be started and completed by the single thread, while an untied
                        task can be migrate between threads during its execution.
                    </body>
                    <marks>
                        <mark type="keyword">thread</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10010">
            <body>a)* Name three potential performance drawbacks in the following code snippet if it is executed on a
                two-socket node.
                <pre>
                    <!-- @formatter:off -->
                    void main() {
                        const int N = 1000000;
                        int a[N];
                        int b[N];
                        for(int i = 0; i &lt; N; i++) {
                            a[i] = i;
                        }
                        #pragma omp parallel for schedule(static, 1)
                        for(int i = 0; i &lt; N; i++) {
                            a[i] = a[i] + 17;
                            b[i] = a[i] % 23;
                        }
                    }
                    <!-- @formatter:on -->
                </pre>
            </body>
            <answers>
                <answer>
                    <body>
                        1: Memory is not initialized on the thread where it is used, causing poor performance on NUMA
                        systems with first-touch.
                        <p/>
                        2: False Sharing may occur because multiple processes are writing to and reading from memory
                        addresses that have close proximity in memory.
                        <p/>
                        3: TODO (options: load imbalance due to static scheduling, RAW dependency inside loop,
                        microscheduling)
                    </body>
                    <marks>
                        <mark type="keyword">init</mark>
                        <mark type="keyword">false</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10011">
            <body>a)* What conditions must hold for a program to be sequentially consistent?</body>
            <answers>
                <answer>
                    <body>
                        As stated in lecture:
                        <pre>
                            [Lamport] “A multiprocessor system is sequentially consistent if the result of any execution
                            is the same as if the operations of all processors were executed in some sequential order,
                            and the operations of each individual processor appear in this sequence in the order
                            specified by the program”
                        </pre>
                    </body>
                    <marks>
                        <mark type="keyword">order</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10012">
            <body>a)* What is the difference between an eager and a rendezvouz protocol?</body>
            <answers>
                <answer>
                    <body>In an eager protocol, the message gets sent directly. This avoids a handshake but might add
                        extra copy operations into/out of unexpected message queues, suitable for short messages. With
                        the rendezvous protocol, only a header is sent. Once the receive has been posted, a message to
                        fetch the actual message is posted. This avoids extra copying at the cost of one additional
                        round-trip, suited for large messages where copying is expensive.
                    </body>
                    <marks>
                        <mark type="keyword">handshake</mark>
                        <mark type="keyword">copy</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10013">
            <body>b)* Describe the functionality of MPI_Waitany</body>
            <answers>
                <answer>
                    <body>When passed an array of MPI_Request objects and a MPI_Status object, MPI_Waitany blocks until
                        any one of the passed requests completes, filling the statuses array variable with status
                        information.
                        <p/>
                        MPI_Waitany(num, requests, statuses);
                    </body>
                    <marks>
                        <mark type="keyword">block</mark>
                        <mark type="keyword">request</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10014">
            <body>a)* Describe the functionality of MPI_Comm_split().</body>
            <answers>
                <answer>
                    <body>MPI_Comm_split(originalComm, color (group), key (order), newComm). Splits the current
                        communicator into one or more new communicators, with processes passing the same color value
                        assigned to the same new communicator, where the key determines the order of the processes in
                        the new communicator. The result is stored in newComm.
                    </body>
                    <marks>
                        <mark type="keyword">color</mark>
                        <mark type="keyword">key</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <!-- TODO: Problem 9 B and C -->
        <question type="extended-answer" id="10015">
            <body>a)* List the blocking collective operation with its parameters that can be used to replace the point
                to point communication between lines 20 to 28.
                <pre>
                    <!-- @formatter:off -->
                    19 // point to point communication
                    20  if(rank == 0)
                    21  for(int r = 1; r &lt; size; r++)
                    22  {
                    23      a[0] = data;
                    24      MPI_Recv(a + r, 1, MPI_FLOAT , r, 0, MPI_COMM_WORLD ,25MPI_STATUS_IGNORE );
                    26  }
                    27  else
                    28      MPI_Send (&amp;data , 1, MPI_FLOAT , 0, 0, MPI_COMM_WORLD );
                    <!-- @formatter:on -->
                </pre>
            </body>
            <answers>
                <answer>
                    <body>
                        Instead of point to point, you can use:
                        <pre>
                            MPI_Gather(&amp;data, 1, MPI_FLOAT, &amp;a, 1 MPI_FLOAT, 0, MPI_COMM_WORLD);
                        </pre>
                    </body>
                    <marks>
                        <mark type="keyword">gather</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10016">
            <body>b)* List the non-blocking collective operation with its parameters that can be used to replace the
                blocking communication from previous subproblem (a) (MPI_Gather).
            </body>
            <answers>
                <answer>
                    <body>
                        Instead of blocking, you can use:
                        <pre>
                            MPI_Request request;
                            MPI_Igather(&amp;data, 1, MPI_FLOAT, &amp;a, 1 MPI_FLOAT, 0, MPI_COMM_WORLD, &amp;request);
                        </pre>
                    </body>
                    <marks>
                        <mark type="keyword">Igather</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10017">
            <body>c)* Name the appropriate operation when using a non-blocking collective to insure its completion and
                specify the proper line number to put this operation for getting the best performance.
                <img src="../data/res/pp/10017_code.png"/>
            </body>
            <answers>
                <answer>
                    <body>
                        Assuming that the MPI_Request is stored in a variable named request:
                        <pre>
                            MPI_Wait(&amp;request, &amp;MPI_STATUS_IGNORE);
                        </pre>
                        This should be placed on line 31.
                    </body>
                    <marks>
                        <mark type="keyword">31</mark>
                        <mark type="keyword">MPI_Wait</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <!-- FAQ -->
        <question type="extended-answer" id="50001">
            <body>What are the two ways to return a value from a kernel in pthreads?</body>
            <answers>
                <answer>
                    <body>Using the void retval** in the function return value (and pthread_join argument). Also
                        possible using an out parameter (pointer) in the function's argument structure.
                    </body>
                    <marks>
                        <mark type="keyword">retval</mark>
                        <mark type="keyword">arg</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="50002">
            <body>What are some work sharing constructs in OpenMP?</body>
            <answers>
                <answer>
                    <body>Sections, for and single</body>
                    <marks>
                        <mark type="keyword">section</mark>
                        <mark type="keyword">for</mark>
                        <mark type="keyword">single</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="50003">
            <body>What are OpenMP default task data scopes?</body>
            <answers>
                <answer>
                    <body>Task region default: firstprivate (unless shared in enclosing construct). Global and static
                        variables (shared). Local variables: private. Orphaned task variables (declared private inside a
                        declared shared and without scoping on task) (firstprivate).
                    </body>
                    <marks>
                        <mark type="keyword">share</mark>
                        <mark type="keyword">private</mark>
                        <mark type="keyword">orphan</mark>
                        <mark type="keyword">static</mark>
                        <mark type="keyword">local</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <!-- TODO: Problem 11 -->

        <!-- Notes: No need to know intrinsic (SIMD) functions -->

        <!-- Some dependence graph tasks -->
        <question type="extended-answer" id="90000">
            <body>
                Give the dependence vectors, direction vectors and draw the dependence graph for the following code
                statement:
                <pre>
                    <!-- @formatter:off -->
                    1   for(i = 1; i &lt; N; i++) {
                    2       for(j = 1; j &lt; M; j++) {
                    3 S1:       A(i + 1,j) = B(i,j + 1)
                    4 S2:       B(i,j) = A(i,j)
                    5       }
                    6   }
                    <!-- @formatter:on -->
                </pre>
            </body>
            <answers>
                <answer>
                    <body>
                        <p/>
                        Student solution!
                        <pre>
                            S1 A(i+1, j) to S2 A(i,j) RAW (true) dependency. Distance (1,0), direction (&lt;, =) level 1
                            S1 B(i, j+1) to S2 B(i,j) WAR (anti) dependency. Distance (0,1), direction (=, &lt;) level 2
                        </pre>
                    </body>
                    <marks></marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="90001">
            <body>
                Give the dependence vectors, direction vectors and draw the dependence graph for the following code
                statement:
                <pre>
                    <!-- @formatter:off -->
                    1   for(i = 0; i &lt; N; i++) {
                    2 S1:   B(i) = A(i)
                    3 S2:   A(i) = A(i) + B(i + 1)
                    4 S3:   C(i) = 2 * B(i)
                    5   }
                    <!-- @formatter:on -->
                </pre>
            </body>
            <answers>
                <answer>
                    <body>
                        S2 (B(i+1)) -> S1 (B(i)): WAR (anti) dependency, distance: (1), direction (&lt;), level = 1
                        <p/>
                        S2 (A(i)) -> S1 (A(i)): WAR (anti) dependency, distance: (0), direction (=), level = \infty
                        <p/>
                        S3 (B(i)) -> S1 (B(i)): RAW (true) dependency, distance: (0), direction (=), level = \infty
                    </body>
                    <marks></marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="90002">
            <body>
                Give the dependence vectors, direction vectors and draw the dependence graph for the following code
                statement:
                <pre>
                    <!-- @formatter:off -->
                    1   for(i = 1; i &lt; N; i++) {
                    2       for(j = 1; j &lt; M; j++) {
                    3 S1:       A(i) = B(i,j)
                    4 S2:       B(i,j) = B(i - 1,2 * j)
                    5       }
                    6   }
                    <!-- @formatter:on -->
                </pre>
            </body>
            <answers>
                <answer>
                    <body>
                        S1 (A(i)) -> S1 (A(i)): WAW (output) dependency, distance: (0, *), direction: (=, *), level = 2
                        <p/>
                        S1 (B(i,j)) -> S2 (B(i,j)): WAR (anti) dependency, distance: (0), direction (=), level = \infty
                        <p/>
                        S2 (B(i,2)) -> S2 (B(i - 1, 2 * j)): RAW (true) dependency, distance: (1, -j), direction (&lt;,
                        >), level = 1
                    </body>
                    <marks></marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="90003">
            <body>
                Give the dependence vectors, direction vectors and draw the dependence graph for the following code
                statement:
                <pre>
                    <!-- @formatter:off -->
                    1   for(i = 0; i &lt; N; i++) {
                    2       for(j = 0; j &lt; M; j++) {
                    3 S1:       B(i - 1,j) = C(i,j - 2)
                    4 S2:       C(i,j) = 2 * B(i,j + 1)
                    5       }
                    6   }
                    <!-- @formatter:on -->
                </pre>
            </body>
            <answers>
                <answer>
                    <body>
                        S2 (B(i,j+1)) -> S1 (B(i-1, j)): WAR (anti) dependency, distance: (1, 1), direction: (&lt;, &lt;),
                        level = 1
                        <p/>
                        S2 (C(i,j)) -> S1 (C(i, j-2)): RAW (true) dependency, distance: (0, 2), direction: (=, &lt;),
                        level = 2
                        <p/>
                    </body>
                    <marks></marks>
                </answer>
            </answers>
        </question>
    </body>
</exam>