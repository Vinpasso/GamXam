<?xml version="1.0" encoding="UTF-8" ?>

<exam xmlns="https://vpt1.org"
      xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
      xsi:schemaLocation="https://vpt1.org questions.xsd">
    <head>
        <title>Parallel Programming</title>
        <version>1.0</version>
        <id>161</id>
    </head>
    <body>
        <!-- TODO: Insert questions -->
        <!-- Lecture 2: Threading Concepts Threading APIs POSIX Threads -->
        <question type="extended-answer" id="1">
            <body>What are the four elements of Flynn's taxonomy?</body>
            <answers>
                <answer>
                    <body>SISD (Conventional processor), SIMD (Single instruction multiple data, vectorization, GPU,
                        etc.), MISD (not really available), MIMD (Multiple instruction multiple data,
                        multi-core/processor etc.)
                    </body>
                    <marks>
                        <mark type="keyword">sisd</mark>
                        <mark type="keyword">simd</mark>
                        <mark type="keyword">misd</mark>
                        <mark type="keyword">mimd</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="2">
            <body>What two major types of MIMD architectures exist?</body>
            <answers>
                <answer>
                    <body>Distributed memory and shared memory</body>
                    <marks>
                        <mark type="keyword">distrib</mark>
                        <mark type="keyword">share</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="3">
            <body>What type of distributed memory systems exist and what do they do?</body>
            <answers>
                <answer>
                    <body>MPP, NOW, Cluster (What do they do? TODO)</body>
                    <marks>
                        <mark type="keyword">mpp</mark>
                        <mark type="keyword">now</mark>
                        <mark type="keyword">clust</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="4">
            <body>What are the types and subtypes of shared memory systems?</body>
            <answers>
                <answer>
                    <body>UMA and NUMA (COMA, ccNUMA, nccNUMA)</body>
                    <marks>
                        <mark type="keyword">UMA</mark>
                        <mark type="keyword">NUMA</mark>
                        <mark type="keyword">COMA</mark>
                        <mark type="keyword">cc</mark>
                        <mark type="keyword">ncc</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="5">
            <body>What is another name for the UMA Architecture? What is the defining feature of a UMA system?</body>
            <answers>
                <answer>
                    <body>SMP: Symmetric multiprocessors. There is a centralized shared memory, to which all processors
                        have the same latency.
                    </body>
                    <marks>
                        <mark type="keyword">smp</mark>
                        <mark type="keyword">shared</mark>
                        <mark type="keyword">latenc</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="6">
            <body>What are some programming models that match shared memory?</body>
            <answers>
                <answer>
                    <body>
                        POSIX threads, OpenMP, etc.
                    </body>
                    <marks>
                        <mark type="keyword">posix</mark>
                        <mark type="regex">o(pen)?mp</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="7">
            <body>What is a thread and what are its essential components?</body>
            <answers>
                <answer>
                    <body>An independent stream of execution that has at least its own PC and its own stack.</body>
                    <marks>
                        <mark type="keyword">exec</mark>
                        <mark type="keyword">pc</mark>
                        <mark type="keyword">stack</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="8">
            <body>What is Hyperthreading/SMT?</body>
            <answers>
                <answer>
                    <body>A technology where a processing core can have multiple hardware threads to allow for rapid
                        context switching in the core, reducing the impact of stalls.
                    </body>
                    <marks>
                        <mark type="keyword">thread</mark>
                        <mark type="keyword">switch</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="9">
            <body>What are some advantages of using threads versus multiple processes.</body>
            <answers>
                <answer>
                    <body>
                        No data protection boundaries, asynchronous behavior within a process, OS still does scheduling
                        (preemption and progress)
                    </body>
                    <marks>
                        <mark type="keyword">protect</mark>
                        <mark type="keyword">async</mark>
                        <mark type="keyword">schedul</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10">
            <body>What is an advantage of using threads (e.g. pthreads) in parallel programming?</body>
            <answers>
                <answer>
                    <body>Its low-level, allowing the programmer to micro-manage resources.</body>
                    <marks>
                        <mark type="keyword">low</mark>
                        <mark type="keyword">resource</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="11">
            <body>What model does pthreads conform to?</body>
            <answers>
                <answer>
                    <body>The fork/join model</body>
                    <marks>
                        <mark type="keyword">fork</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="12">
            <body>Write code to create/destroy a pthread.</body>
            <answers>
                <answer>
                    <body>
                        Using a kernel:
                        <pre>void* kernel(void* arg);</pre>
                        Start a thread:
                        <pre>
                            pthread_t thread;
                            pthread_attr_t attr;
                            if(!pthread_create(&amp;thread, attr, kernel, arg)) {
                            abort(0);
                            }
                        </pre>
                        Join a thread:
                        <pre>
                            if(!pthread_join(&amp;thread, &amp;retval)) {
                            abort(0);
                            }
                        </pre>
                    </body>
                    <marks>
                        <mark type="keyword">create</mark>
                        <mark type="keyword">join</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="13">
            <body>What is a potential issue when using pthreads concerning memory?</body>
            <answers>
                <answer>
                    <body>The stacks of multiple threads need to be stored in the same address space. Usually, this is
                        done with a pre-defined space between each stack, limiting the amount of available stack space.
                    </body>
                    <marks>
                        <mark type="keyword">stack</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="14">
            <body>What are the two main concepts for synchronization between threads in POSIX?</body>
            <answers>
                <answer>
                    <body>Mutual exclusion, condition variables</body>
                    <marks>
                        <mark type="keyword">excl</mark>
                        <mark type="keyword">condit</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="15">
            <body>Which problem is solved by the mutual exclusion concept?</body>
            <answers>
                <answer>
                    <body>Concurrent access to shared resources (IO, Memory, etc.)</body>
                    <marks>
                        <mark type="keyword">resourc</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="16">
            <body>Write code for one lifecycle of a POSIX Mutex.</body>
            <answers>
                <answer>
                    <body>
                        Initialization (to use default attributes, use NULL):
                        <pre>
                            //Dynamic case, destruction necessary
                            pthread_mutex_t lock;
                            pthread_mutex_init(&amp;lock, attr);
                            //Static case
                            pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;
                        </pre>
                        Lock:
                        <pre>
                            pthread_mutex_lock(&amp;lock);
                            pthread_mutex_trylock(&amp;lock);
                        </pre>
                        Unlock:
                        <pre>
                            pthread_mutex_unlock(&amp;lock);
                        </pre>
                        Destroy:
                        <pre>
                            pthread_mutex_destroy(&amp;lock);
                        </pre>
                    </body>
                    <marks></marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="17">
            <body>What types of recursive behaviors are available for pthread mutexes?</body>
            <answers>
                <answer>
                    <body>PTHREAD_MUTEX_NORMAL (deadlock), PTHREAD_MUTEX_ERRORCHECK (error code),
                        PTHREAD_MUTEX_RECURSIVE (lock count). Default: undefined behavior
                    </body>
                    <marks>
                        <mark type="keyword">normal</mark>
                        <mark type="keyword">error</mark>
                        <mark type="keyword">recur</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="18">
            <body>What criteria are there for the implementation of a pthread mutex?</body>
            <answers>
                <answer>
                    <body>Guarantee of mutual exclusion, progress (every waiting thread eventually gets the mutex),
                        fairness.
                    </body>
                    <marks>
                        <mark type="keyword">excl</mark>
                        <mark type="keyword">prog</mark>
                        <mark type="keyword">fair</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="19">
            <body>What are the two typical implementations for a pthread mutex and what do they do?</body>
            <answers>
                <answer>
                    <body>
                        Spin-lock: Try to obtain lock in a loop as fast as possible, enable low latency but consuming
                        resources.
                        <p/>
                        Yielding lock: Yields the hardware thread when lock unavailable, freeing resources but
                        increasing latency.
                    </body>
                    <marks>
                        <mark type="keyword">spin</mark>
                        <mark type="keyword">yield</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="20">
            <body>What implementations of a mutex exist with/without hardware support?</body>
            <answers>
                <answer>
                    <body>HW Support: Locking using atomic operations, such as test and set or compare and swap. No HW
                        Support: Using e.g. Petersen's algorithm.
                    </body>
                    <marks>
                        <mark type="keyword">atomic</mark>
                        <mark type="keyword">petersen</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="21">
            <body>What is the tradeoff with lock granularity?</body>
            <answers>
                <answer>
                    <body>Too fine and a significant portion of compute time will be spent on obtaining and releasing
                        locks. Too coarse and threads will need to wait for accessing resources that could be accessed
                        concurrently.
                    </body>
                    <marks>
                        <mark type="keyword">time</mark>
                        <mark type="keyword">resourc</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="22">
            <body>When does a deadlock occur?</body>
            <answers>
                <answer>
                    <body>When there is a cyclic dependency when obtaining locks.</body>
                    <marks>
                        <mark type="keyword">cycl</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="23">
            <body>What approaches exist to resolve the deadlock problem and what is a drawback of each?</body>
            <answers>
                <answer>
                    <body>One lock (very coarse). Arbiter (complex central instance, scalability). Lock order
                        (fairness). Custom/hybrid schemes.
                    </body>
                    <marks>
                        <mark type="keyword">one</mark>
                        <mark type="keyword">arbit</mark>
                        <mark type="keyword">order</mark>
                        <mark type="keyword">hybrid</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="24">
            <body>What is the concept behind condition variables for synchronization?</body>
            <answers>
                <answer>
                    <body>One thread blocks while the other thread sends a signal once the condition is met.</body>
                    <marks>
                        <mark type="keyword">block</mark>
                        <mark type="keyword">signal</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="25">
            <body>How are pthread condition variables used (write one lifecycle of code)?</body>
            <answers>
                <answer>
                    <body>
                        Lifecycle:
                        <pre>
                            //Static
                            pthread_mutex_t mutex = PTHREAD_MUTEX_INITIALIZER;
                            pthread_cond_t cond = PTHREAD_COND_INITIALIZER;

                            ...
                            //Waiting Thread
                            pthread_mutex_lock(&amp;lock);
                            //Do work
                            // Make sure to guard against sporadic wakes
                            while(!CONDITION) {
                            // This unlocks and relocks mutex implicitly so other threads have access
                            pthread_cond_wait(&amp;cond, &amp;mutex);
                            }
                            //Do work
                            pthread_mutex_unlock(&amp;lock);

                            ...
                            //Signal thread
                            pthread_mutex_lock(&amp;lock);
                            //Work
                            CONDITION = true;
                            pthread_cond_signal(&amp;cond);
                            pthread_mutex_unlock(&amp;mutex);
                        </pre>
                    </body>
                    <marks>
                        <mark type="keyword">lock</mark>
                        <mark type="keyword">wait</mark>
                        <mark type="keyword">signal</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="26">
            <body>What are some relevant performance aspects of threading?</body>
            <answers>
                <answer>
                    <body>Overhead of thread management (creation/destruction, use thread pools). Lock contention with
                        many threads. Pinning of threads. Good use of cache and avoiding false sharing.
                    </body>
                    <marks>
                        <mark type="keyword">thread</mark>
                        <mark type="keyword">lock</mark>
                        <mark type="keyword">pin</mark>
                        <mark type="keyword">cache</mark>
                        <mark type="keyword">shar</mark>
                    </marks>
                </answer>
            </answers>
        </question>

        <!-- Chapter 3 -->
        <question type="extended-answer" id="27">
            <body>What were the design goals of OpenMP?</body>
            <answers>
                <answer>
                    <body>Standard for writing parallel programs (mainly on-node), "lean and mean" (simple API for
                        complex goals), ease of use and portability
                    </body>
                    <marks>
                        <mark type="keyword">standard</mark>
                        <mark type="keyword">lean</mark>
                        <mark type="keyword">ease</mark>
                        <mark type="keyword">port</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="28">
            <body>What is OpenMP?</body>
            <answers>
                <answer>
                    <body>An Application Program Interface (API) used to program multi-threaded shared memory
                        parallelism. It is comprised of 3 components: compiler directives, a runtime library, and
                        environment variables.
                    </body>
                    <marks>
                        <mark type="keyword">API</mark>
                        <mark type="keyword">share</mark>
                        <mark type="keyword">component</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="29">
            <body>How does OpenMP correspond to the Fork/Join Model?</body>
            <answers>
                <answer>
                    <body>
                        Step 1: Sequential execution.
                        Step 2: Begin of parallel region, spawning threads
                        Step 3: Parallel computation.
                        Step 4: Synchronization at end of parallel region (implicit/explicit)
                        Step 5: Sequential execution.
                    </body>
                    <marks>
                        <mark type="keyword">region</mark>
                        <mark type="keyword">sequen</mark>
                        <mark type="keyword">parall</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="30">
            <body>What are some misconceptions about nested parallel regions?</body>
            <answers>
                <answer>
                    <body>OpenMP is not required to use more threads (can just continue executing sequentially), mapping
                        to HW threads is runtime dependent.
                    </body>
                    <marks>
                        <mark type="keyword">sequen</mark>
                        <mark type="keyword">HW</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="31">
            <body>What is needed for an OpenMP program?</body>
            <answers>
                <answer>
                    <body>A compiler with OpenMP support as well as a runtime system (usually based on pthreads).
                        Behavior can be controlled by ICVs (internal control variables).
                    </body>
                    <marks>
                        <mark type="keyword">compile</mark>
                        <mark type="keyword">runtime</mark>
                        <mark type="keyword">variable</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="32">
            <body>What do OpenMP compiler directives apply to?</body>
            <answers>
                <answer>
                    <body>A structured block ("{}") with a single entry and exit point.</body>
                    <marks>
                        <mark type="keyword">block</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="33">
            <body>Which OpenMP directive is used to make sure multiple pieces of code are executed once by one thread in
                parallel? Where is the synchronization point?
            </body>
            <answers>
                <answer>
                    <body>A section:
                        <pre>
                            0 #pragma omp parallel
                            1 {
                            2 #pragma omp sections
                            3 {
                            4 #pragma omp section
                            5 { block }
                            6 #pragma omp section
                            7 { block }
                            8 }
                            9 }
                        </pre>
                        Implicit barrier at the end of a sections block.
                    </body>
                    <marks>
                        <mark type="keyword">section</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="34">
            <body>Which OpenMP directive is used to parallelize pieces of a loop? What condition needs to hold and are
                there synchronization points?
            </body>
            <answers>
                <answer>
                    <body>
                        Inside a parallel region, use the following to distribute iterations across threads:
                        <pre>#pragma omp for</pre>
                        Condition: All iterations must be independent. Synchronization happens implicitly at end of
                        loop.
                    </body>
                    <marks>
                        <mark type="keyword">for</mark>
                        <mark type="keyword">independen</mark>
                        <mark type="keyword">sync</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="35">
            <body>What are the available OpenMP Loop schedules and what do they do?</body>
            <answers>
                <answer>
                    <body>Static (preassigned, default size ~n/t), dynamic (fixed size, default 1), guided (chunks
                        decreasing in size), runtime (using environment variable), auto (let OpenMP choose).
                    </body>
                    <marks>
                        <mark type="keyword">static</mark>
                        <mark type="keyword">dynamic</mark>
                        <mark type="keyword">guide</mark>
                        <mark type="keyword">runtime</mark>
                        <mark type="keyword">auto</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="36">
            <body>What is the final value of i in the following code snippet and why?
                <pre>
                    int i = 3;
                    #pragma omp parallel for lastprivate(i)
                    for(int j=0; j&lt;4; j++) {
                    i=i+1;
                    printf("-> i=%d\n", i);
                    }
                    printf("FinalValueof I=%d\n", i);
                </pre>
            </body>
            <answers>
                <answer>
                    <body>Undefined, because the private variable is not initialized.</body>
                    <marks>
                        <mark type="keyword">undef</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="37">
            <body>What is the behavior of the sharing attribute default(none)?</body>
            <answers>
                <answer>
                    <body>All variables need to be explicitly set to shared or private, no implicit sharing.</body>
                    <marks>
                        <mark type="keyword">explicit</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="38">
            <body>What are the available OpenMP reduction operators and how is a reduction written? What limitation is
                placed on the variable?
            </body>
            <answers>
                <answer>
                    <body>With the following clause:
                        <pre>
                            reduction(operator: list)
                        </pre>
                        Where operator is one of the following:
                        <pre>
                            arithmetic: +, *, - (alert: subtract in local part, then thread results added together),
                            min, max
                            logical: &amp;, ^, |, &amp;&amp;, ||
                            user defined (newer versions)
                        </pre>
                        The variable is only allowed to show up in the following operations:
                        <pre>
                            x = x operator expr
                            x operator= expr
                            x++, ++x, x--, --x
                        </pre>
                    </body>
                    <marks>
                        <mark type="keyword">reduc</mark>
                        <mark type="keyword">op</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="39">
            <body>What is the difference between an OpenMP master region, single region, critical section, and atomic
                statement?
            </body>
            <answers>
                <answer>
                    <body>
                        A master region enforces that only the (specific) master thread executes the code block (no
                        synchronization at beginning).
                        <p/>
                        A single region allows any one thread to execute the region, others skipping (implicit sync at
                        end)
                        <p/>
                        A critical region is a mutually exclusive block executed by all threads (use naming if multiple,
                        all unnamed mapped to same name.
                        <p/>
                        An atomic statement is like a one statement critical block (limited to x operator= expr,
                        x++, x--, ...) that may be implemented in hardware to avoid locking.
                    </body>
                    <marks>
                        <mark type="keyword">thread</mark>
                        <mark type="keyword">excl</mark>
                        <mark type="keyword">critical</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="40">
            <body>What constructs does OpenMP provide for locking?</body>
            <answers>
                <answer>
                    <body>Simple locks using omp_init_lock(&amp;lock), omp_destroy_lock(&amp;lock), omp_set_lock(&amp;lock),
                        omp_unset_lock(&amp;lock), res = omp_test_lock(&amp;lock) (checks and possibly sets lock,
                        returning true if acquired).
                        <p/>
                        Similar routines for nestable locks.
                    </body>
                    <marks>
                        <mark type="keyword">simple</mark>
                        <mark type="keyword">nestable</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="41">
            <body>What does the OpenMP ordered construct do and how is it used?</body>
            <answers>
                <answer>
                    <body>Ensures that the ordered block (S2) is executed in sequential order. Blocks outside that
                        region may float.
                        <pre>
                            1 #pragma omp for ordered
                            2 for (...){
                            3 S1
                            4 #pragma omp ordered
                            5 {
                            6 S2
                            7 }
                            8 S3
                            9 }
                        </pre>
                        <img src="../data/res/pp/42_sequence.png"/>
                    </body>
                    <marks>
                        <mark type="keyword">sequen</mark>
                    </marks>
                </answer>
            </answers>
        </question>

        <question type="extended-answer" id="42">
            <body>What is the difference between omp_get_max_threads() and omp_get_num_threads()?</body>
            <answers>
                <answer>
                    <body>omp_get_max_threads determines the maximum number of threads for team creation, omp get num
                        threads determines the number of threads in the current team (1 in sequential code).
                    </body>
                    <marks>
                        <mark type="keyword">team</mark>
                        <mark type="keyword">sequen</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="43">
            <body>What are some drawbacks of work sharing in OpenMP? What is a possible solution?</body>
            <answers>
                <answer>
                    <body>Imbalance caused by workload (especially static, NUMA), and machine (differences in
                        environment). Limited programming flexibility (sections/for loops) not suited for hierarchical
                        parallelism. Explicit tasking can be used as a solution.
                    </body>
                    <marks>
                        <mark type="keyword">work</mark>
                        <mark type="keyword">machine</mark>
                        <mark type="keyword">hierarch</mark>
                        <mark type="keyword">task</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="44">
            <body>What is a task in OpenMP?</body>
            <answers>
                <answer>
                    <body>An independent piece of work (that is guaranteed to be executed, in any order). They are
                        scheduled using task queues and dispatched to threads.
                    </body>
                    <marks>
                        <mark type="keyword">independen</mark>
                        <mark type="keyword">work</mark>
                        <mark type="keyword">order</mark>
                        <mark type="keyword">queue</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="45">
            <body>What is the difference between a tied and an untied task?</body>
            <answers>
                <answer>
                    <body>A tied task is bound to a thread once it starts and can no longer move, while an untied task
                        might be interrupted and moved elsewhere, offering more flexibility and resource utilization.
                    </body>
                    <marks>
                        <mark type="keyword">thread</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="46">
            <body>How can OpenMP tasks be used to split work?</body>
            <answers>
                <answer>
                    <body>
                        By using the task construct inside a parallel region. Make sure that you don't spawn more tasks
                        than intended (multiple threads).
                        <pre>
                            #pragma omp parallel
                            {
                            #pragma omp single
                            {
                            for ( elem = l->first; elem; elem= elem->next)
                            #pragma omp task
                            process(elem)
                            }
                            // all tasks are complete by this point
                            }
                        </pre>
                    </body>
                    <marks>
                        <mark type="keyword">task</mark>
                        <mark type="keyword">single</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="47">
            <body>
                What does the following clause do if condition is true/false?
                <pre>
                    #pragma omp task if(CONDITION)
                    { ... }
                </pre>
            </body>
            <answers>
                <answer>
                    <body>If true, creates the task normally. If false, the creating thread executes the new task
                        immediately, and cannot resume the current task without completing the new one.
                    </body>
                    <marks>
                        <mark type="keyword">task</mark>
                        <mark type="keyword">thread</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="48">
            <body>
                What does the following clause do?
                <pre>
                    #pragma omp taskwait
                </pre>
            </body>
            <answers>
                <answer>
                    <body>Waits for the completion of immediate child tasks, where child tasks are tasks generated since
                        the beginning of the current task.
                    </body>
                    <marks>
                        <mark type="keyword">child</mark>
                        <mark type="keyword">task</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="49">
            <body>What are some explicit/implicit task scheduling points in OpenMP?</body>
            <answers>
                <answer>
                    <body>
                        Explicit (standalone directive):
                        <pre>
                            #pragma omp taskyield
                        </pre>
                        Implicit: task creation, end of task, taskwait, barrier
                    </body>
                    <marks>
                        <mark type="keyword">yield</mark>
                        <mark type="keyword">creat</mark>
                        <mark type="keyword">end</mark>
                        <mark type="keyword">wait</mark>
                        <mark type="keyword">barrier</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="50">
            <body>What 3 types of OpenMP constructs are there?</body>
            <answers>
                <answer>
                    <body>Worksharing constructs (loops, sections, single), tasking constructs (task, taskyield),
                        master/synchronization constructs (master, critical, barrier, taskwait, atomic, flush, ordered)
                    </body>
                    <marks>
                        <mark type="keyword">workshar</mark>
                        <mark type="keyword">task</mark>
                        <mark type="keyword">sync</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="51">
            <body>How are OpenMP task dependencies used, how do they work?</body>
            <answers>
                <answer>
                    <body>Using the depend clause:
                        <pre>#pragma omp task shared(...) depend(in/out/inout: var)</pre>
                        in: depends on all previously generated tasks with out/inout.
                        <p/>
                        out: depends on all previously generated tasks with in/out/inout
                        <p/>
                    </body>
                    <marks>
                        <mark type="keyword">depend</mark>
                        <mark type="keyword">in</mark>
                        <mark type="keyword">out</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="52">
            <body>What are some advantages and performance considerations in OpenMP tasking?</body>
            <answers>
                <answer>
                    <body>
                        Advantages: implicit load balancing, simple programming model, complications/bookkeeping pushed
                        to runtime.
                        <p/>
                        Considerations: Granularity and NUMA optimizaton.
                    </body>
                    <marks>
                        <mark type="keyword">balanc</mark>
                        <mark type="keyword">model</mark>
                        <mark type="keyword">runtime</mark>
                        <mark type="keyword">granular</mark>
                        <mark type="keyword">NUMA</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="53">
            <body>What is the difference between memory/cache coherency and memory consistency?</body>
            <answers>
                <answer>
                    <body>
                        Coherency: reason about updates to one memory location.
                        <p/>
                        Consistency: reason about updates to several memory locations.
                        <p/>
                    </body>
                    <marks>
                        <mark type="keyword">location</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="54">
            <body>When is a system coherent?</body>
            <answers>
                <answer>
                    <body>If program order for loads/stores is preserved, all stores eventually become visible, all
                        processors see same order of writes.
                    </body>
                    <marks>
                        <mark type="keyword">order</mark>
                        <mark type="keyword">visib</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="54">
            <body>What ways are there to establish coherency?</body>
            <answers>
                <answer>
                    <body>Using snoop-based or directory based protocols.</body>
                    <marks>
                        <mark type="keyword">snoop</mark>
                        <mark type="keyword">directory</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="55">
            <body>What is sequential consistency?</body>
            <answers>
                <answer>
                    <body>[Lamport] “A multiprocessor system is sequentially consistent if the result of any execution
                        is the same as if the operations of all processors were executed in some sequential order, and
                        the operations of each individual processor appear in this sequence in the order specified by
                        the program”
                        <p/>
                        In other words, each processor issues memory instructions in program order that can be
                        sequentialized in any way that does not violate a processor's program order.
                    </body>
                    <marks>
                        <mark type="keyword">program</mark>
                        <mark type="keyword">order</mark>
                        <mark type="keyword">processor</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="56">
            <body>Why is sequential consistency hard?</body>
            <answers>
                <answer>
                    <body>Reordering elements such as compiler optimizations, instruction reordering, network elements,
                        etc. Write has to be atomic with respect to all processors (all need to see effect immediately).
                    </body>
                    <marks>
                        <mark type="keyword">order</mark>
                        <mark type="keyword">atom</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="57">
            <body>What is processor consistency and how is it weaker than sequential consistency?</body>
            <answers>
                <answer>
                    <body>Writes by any thread seen by all threads in order they were issued. But: Different threads may
                        see different orders.
                    </body>
                    <marks>
                        <mark type="keyword">issue</mark>
                        <mark type="keyword">order</mark>
                        <mark type="keyword">thread</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="58">
            <body>What is weak consistency and how is it weaker than sequential consistency?</body>
            <answers>
                <answer>
                    <body>In weak consistency, there are data operations and synchronization operations. Only
                        synchronization operations are sequentially consistent (flushing the memory pipeline).
                    </body>
                    <marks>
                        <mark type="keyword">data</mark>
                        <mark type="keyword">synchronization</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="59">
            <body>What is release consistency and how is it weaker than sequential consistency?</body>
            <answers>
                <answer>
                    <body>Subdivides synchronization operations into acquire and release (corresponding to lock
                        operations). Only acquires/releases have to be sequentially consistent.
                    </body>
                    <marks>
                        <mark type="keyword">lock</mark>
                        <mark type="keyword">sequen</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="58">
            <body>What consistency model does OpenMP loosely conform to, what does that mean?</body>
            <answers>
                <answer>
                    <body>Weak consistency. Only synchronization operations are sequentially consistent.</body>
                    <marks>
                        <mark type="keyword">weak</mark>
                        <mark type="keyword">sync</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="59">
            <body>
                Which of the following are not memory synchronization points (flush points)?
                <p/>
                <pre>
                    •Entry and exit of critical regions
                    •Barriers (implicit and explicit)
                    •Entry and exit of master regions
                    •Entry and exit of work sharing regions
                    •Every task scheduling point
                    •Use of OpenMP runtime locks
                    •Entry and exit of parallel regions
                </pre>
            </body>
            <answers>
                <answer>
                    <body>
                        <pre>
                            •Entry and exit of work sharing regions
                            •Entry and exit of master regions
                        </pre>
                    </body>
                    <marks>
                        <mark type="keyword">shar</mark>
                        <mark type="keyword">master</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="60">
            <body>
                What are potential problems with the following code? What construct is used to solve this?
                <p/>
                <pre>
                    //Thread 1
                    a = foo();
                    flag = 1;

                    //Thread 2
                    while (flag);
                    b = a;
                </pre>
            </body>
            <answers>
                <answer>
                    <body>Instruction reordering by compiler (doesn't see dependency). Variables might be kept in
                        registers. Need the flush directive to ensure that changes are visible and consistent.
                        <p/>
                        <pre>
                            //Thread 1
                            a = foo();
                            #pragma omp flush(a, flag)
                            flag = 1;
                            #pragma omp flush(flag)

                            //Thread 2
                            while (flag)
                            {
                            #pragma omp flush(flag)
                            }
                            #pragma omp flush(a, flag)
                            b = a;
                        </pre>
                    </body>
                    <marks>
                        <mark type="keyword">order</mark>
                        <mark type="keyword">register</mark>
                        <mark type="keyword">flush</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="61">
            <body>When is there a control dependency between S1 and S2?</body>
            <answers>
                <answer>
                    <body>
                        Iff:
                        <p/>
                        There exists an execution path from S1 to S2
                        <p/>
                        Not all execution paths lead from S1 to S2
                    </body>
                    <marks>
                        <mark type="keyword">path</mark>
                        <mark type="keyword">exist</mark>
                        <mark type="keyword">all</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="62">
            <body>When is there a data dependency between S1 and S2?</body>
            <answers>
                <answer>
                    <body>
                        Iff:
                        <p/>
                        There exists an execution path from S1 to S2
                        <p/>
                        S1 and S2 access the same data, at least one is a write.
                    </body>
                    <marks>
                        <mark type="keyword">path</mark>
                        <mark type="keyword">data</mark>
                        <mark type="keyword">write</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <!-- TODO: Continue Chapter 4 Slide 32 -->


        <!-- From Exam 2018 -->
        <question type="extended-answer" id="1000">
            <body>1a) What can be computed by applying Amdahl's Law [and state the law]?</body>
            <answers>
                <answer>
                    <body>Using Amdahl's Law, we can compute the minimum achievable runtime in a program
                        consisting of sequential and parallel regions for a given number of processors.
                        Amdahl's Law states that <pre>T(p)= (1-f)T + f(T/p)</pre>, where f is fraction of parallel
                        execution and p number of processes. From this, it is possible to derive the maximum speedup
                        S(p)=1/(1-f+f/p).
                    </body>
                    <marks>
                        <mark type="keyword">speedup</mark>
                        <mark type="keyword">sequen</mark>
                        <mark type="keyword">par</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10001">
            <body>1b) Assume you have a program, which is split into regions that either cannot be parallelized at all
                or that can be parallelized perfectly. Assuming the regions that cannot be parallelized consume 1% of
                execution time during a sequential run, what is the maximal speedup that can be achieved?
            </body>
            <answers>
                <answer>
                    <body>Using Amdahl's Law with f=0.99, the maximum achievable speedup is 1/(1-f+f/p), with p very
                        large. Thus 1/(1-0.99) = 1/0.01 = 100.
                    </body>
                    <marks>
                        <mark type="keyword">100</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10002">
            <body>2a) Name one possible reason for a negative performance impact in case lock granularity is chosen too
                fine.
            </body>
            <answers>
                <answer>
                    <body>When lock granularity is too fine there are many lock/unlock operations when accessing
                        resources, the processing of which detracts from the time spent on useful computation.
                    </body>
                    <marks>
                        <mark type="keyword">lock</mark>
                        <mark type="keyword">time</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10003">
            <body>2b) Name one possible reason for a negative performance impact in case lock granularity is chosen too
                coarse:
            </body>
            <answers>
                <answer>
                    <body>When lock granularity is too coarse, accessing independent resources (such as different
                        locations in a large array) can block just because they are protected by the same lock (for the
                        whole array). This requires a process to wait even though the resource could be accessed.
                    </body>
                    <marks>
                        <mark type="keyword">lock</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10004">
            <body>3a) Parallelize for loop in the following function using OpenMP by replacing &gt;OPENMP HERE&lt;with
                the appropriate OpenMP directive(s) and clause(s). Write your solution in the box below.
                <pre>
                    void function(float* a,float* b,int n) {
                    int i;
                    &lt;OPENMP HERE&gt;
                    for(i = 0; i &lt; n - 1; i++)
                    a[i] = (b[i] + b[i + 1]) / 2;
                    a[i] = b[i] / 2;
                    }
                </pre>
            </body>
            <answers>
                <answer>
                    <body>
                        <pre>
                            #pragma omp parallel for private(i) shared(a, b, n)
                        </pre>
                    </body>
                    <marks>
                        <mark type="keyword">private</mark>
                        <mark type="keyword">shared</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10005">
            <body>b)* Parallelize for loop in the following function using OpenMP by replacing &gt;OPENMP HERE&lt; with
                the appropriate OpenMP directive(s) and clause(s). Write your solution in the box below.
                <pre>
                    double function(float* x,int* y,int n)
                    {
                    int i, b = y[0];
                    float a = 1.0;
                    &gt;OPENMP HERE&lt;
                    for(i=0; i &gt; n; i++) {
                    a += x[i];
                    if(b &lt; y[i])
                    b = y[i];
                    }
                    return a * b;
                    }
                </pre>
            </body>
            <answers>
                <answer>
                    <body>#pragma omp parallel for private(i) reduction(+: a) reduction(min: b) shared(x, y)</body>
                    <marks>
                        <mark type="keyword">reduction</mark>
                        <mark type="keyword">+</mark>
                        <mark type="keyword">min</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10006">
            <body>
                Consider the following program being executed with OMP_NUM_THREADS set to 16.
                <pre>
                    int value[64], k, t;
                    for(inti=0; i&lt;64; i++)
                    value[i]=63-i;
                    t=omp_get_thread_num ();
                    k=42;
                    #pragma omp parallel for schedule(static, 2) CLAUSE
                    for(inti=0; i&lt;64; i++) {
                    k=value[i]*2+t;
                    }
                    printf("Final␣value␣of␣k=%i\n", k);
                </pre>
                a)* Which iterations are executed by thread 13 in the parallel for loop?
            </body>
            <answers>
                <answer>
                    <body>Iterations 26, 27 and 58, 59</body>
                    <marks>
                        <mark type="keyword">26</mark>
                        <mark type="keyword">27</mark>
                        <mark type="keyword">58</mark>
                        <mark type="keyword">59</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10007">
            <body>
                Consider the following program being executed with OMP_NUM_THREADS set to 16.
                <pre>
                    int value[64], k, t;
                    for(inti=0; i&lt;64; i++)
                    value[i]=63-i;
                    t=omp_get_thread_num ();
                    k=42;
                    #pragma omp parallel for schedule(static, 2) CLAUSE
                    for(inti=0; i&lt;64; i++) {
                    k=value[i]*2+t;
                    }
                    printf("Final␣value␣of␣k=%i\n", k);
                </pre>
                b)* Fill in the following table. If multiple values are possible, state MULTIPLE.
                <pre>
                    CLAUSE set to | Printed value of k
                    NONE |
                    private(k,t) |
                    shared(k,t) |
                    firstprivate(k,t) |
                    lastprivate(k,t) |
                    firstprivate(k,t) |
                    lastprivate(k,t) |
                </pre>
            </body>
            <answers>
                <answer>
                    <body>
                        <pre>
                            CLAUSE set to | Printed value of k
                            NONE | MULTIPLE (default shared)
                            private(k,t) | 42
                            shared(k,t) | MULTIPLE
                            firstprivate(k,t) | 42
                            lastprivate(k,t) | MULTIPLE (due to uninitialized, observed only 0)
                            firstprivate(k,t) lastprivate(k,t) | 0
                        </pre>
                    </body>
                    <marks>
                        <mark type="keyword">MULTIPLE</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10008">
            <body>
                a)* List the OpenMP pragma used to specify a task [and the clause to ensure a task completes]:
            </body>
            <answers>
                <answer>
                    <body>
                        <pre>
                            #pragma omp task
                            #pragma omp taskwait
                        </pre>
                    </body>
                    <marks>
                        <mark type="keyword">task</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10009">
            <body>b)* What is the difference between a tied and an untied task?</body>
            <answers>
                <answer>
                    <body>A tied task is guaranteed to be started and completed by the single thread, while an untied
                        task can be migrate between threads during its execution.
                    </body>
                    <marks>
                        <mark type="keyword">thread</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10010">
            <body>a)* Name three potential performance drawbacks in the following code snippet if it is executed on a
                two-socket node.
                <pre>
                    void main() {
                    const int N = 1000000;
                    int a[N];
                    int b[N];
                    for(int i = 0; i &gt; N; i++) {
                    a[i] = i;
                    }
                    #pragma omp parallel for schedule(static, 1)
                    for(int i = 0; i &gt; N; i++) {
                    a[i] = a[i] + 17;
                    b[i] = a[i] % 23;
                    }
                    }
                </pre>
            </body>
            <answers>
                <answer>
                    <body>
                        1: Memory is not initialized on the thread where it is used, causing poor performance on NUMA
                        systems with first-touch.
                        <p/>
                        2: False Sharing may occur because multiple processes are writing to and reading from memory
                        addresses that have close proximity in memory.
                        <p/>
                        3: TODO (options: load imbalance due to static scheduling, RAW dependency inside loop,
                        microscheduling)
                    </body>
                    <marks>
                        <mark type="keyword">init</mark>
                        <mark type="keyword">false</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10011">
            <body>a)* What conditions must hold for a program to be sequentially consistent?</body>
            <answers>
                <answer>
                    <body>
                        As stated in lecture:
                        <pre>
                            [Lamport] “A multiprocessor system is sequentially consistent if the result of any execution
                            is the same as if the operations of all processors were executed in some sequential order,
                            and the operations of each individual processor appear in this sequence in the order
                            specified by the program”
                        </pre>
                    </body>
                    <marks>
                        <mark type="keyword">order</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10012">
            <body>a)* What is the difference between an eager and a rendevouz protocol?</body>
            <answers>
                <answer>
                    <body>In an eager protocol, the message gets sent directly. This avoids a handshake but might add
                        extra copy operations into/out of unexpected message queues, suitable for short messages. With
                        the rendezvous protocol, only a header is sent. Once the receive has been posted, a message to
                        fetch the actual message is posted. This avoids extra copying at the cost of one additional
                        round-trip, suited for large messages where copying is expensive.
                    </body>
                    <marks>
                        <mark type="keyword">handshake</mark>
                        <mark type="keyword">copy</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10013">
            <body>b)* Describe the functionality of MPI_Waitany</body>
            <answers>
                <answer>
                    <body>When passed an array of MPI_Request objects and a MPI_Status object, MPI_Waitany blocks until
                        any one of the passed requests completes, filling the status variable with status information.
                    </body>
                    <marks>
                        <mark type="keyword">block</mark>
                        <mark type="keyword">request</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10014">
            <body>a)* Describe the functionality of MPI_Comm_split().</body>
            <answers>
                <answer>
                    <body>MPI_Comm_split(originalComm, color (group), key (order), newComm). Splits the current
                        communicator into one or more new communicators, with processes passing the same color value
                        assigned to the same new communicator, where the key determines the order of the processes in
                        the new communicator. The result is stored in newComm.
                    </body>
                    <marks>
                        <mark type="keyword">color</mark>
                        <mark type="keyword">key</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <!-- TODO: Problem 9 B and C -->
        <question type="extended-answer" id="10015">
            <body>a)* List the blocking collective operation with its parameters that can be used to replace the point
                to point communication between lines 20 to 28.
                <pre>
                    19 // point to point communication
                    20 if(rank == 0)
                    21 for(int r = 1; r &lt; size; r++)
                    22 {
                    23 a[0] = data;
                    24 MPI_Recv(a + r, 1, MPI_FLOAT , r, 0, MPI_COMM_WORLD ,25MPI_STATUS_IGNORE );
                    26 }
                    27 else
                    28 MPI_Send (&amp;data , 1, MPI_FLOAT , 0, 0, MPI_COMM_WORLD );
                </pre>
            </body>
            <answers>
                <answer>
                    <body>
                        Instead of point to point, you can use:
                        <pre>
                            MPI_Gather(&amp;data, 1, MPI_FLOAT, &amp;a, 1 MPI_FLOAT, 0, MPI_COMM_WORLD);
                        </pre>
                    </body>
                    <marks>
                        <mark type="keyword">gather</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10016">
            <body>b)* List the non-blocking collective operation with its parameters that can be used to replace the
                blocking communication from previous subproblem (a) (MPI_Gather).
            </body>
            <answers>
                <answer>
                    <body>
                        Instead of blocking, you can use:
                        <pre>
                            MPI_Request request;
                            MPI_Igather(&amp;data, 1, MPI_FLOAT, &amp;a, 1 MPI_FLOAT, 0, MPI_COMM_WORLD, &amp;request);
                        </pre>
                    </body>
                    <marks>
                        <mark type="keyword">Igather</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10017">
            <body>c)* Name the appropriate operation when using a non-blocking collective to insure its completion and
                specify the proper line number to put this operation for getting the best performance.
                <img src="../data/res/pp/10017_code.png"/>
            </body>
            <answers>
                <answer>
                    <body>
                        Assuming that the MPI_Request is stored in a variable named request:
                        <pre>
                            MPI_Wait(&amp;request, &amp;MPI_STATUS_IGNORE);
                        </pre>
                        This should be placed on line 31.
                    </body>
                    <marks>
                        <mark type="keyword">31</mark>
                        <mark type="keyword">MPI_Wait</mark>
                    </marks>
                </answer>
            </answers>
        </question>
    </body>
    <!-- TODO: Problem 11 -->
</exam>