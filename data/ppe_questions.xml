<?xml version="1.0" encoding="UTF-8" ?>

<exam xmlns="https://vpt1.org"
      xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
      xsi:schemaLocation="https://vpt1.org questions.xsd">
    <head>
        <title>Parallel Program Engineering Questions</title>
        <version>1.0</version>
        <id>4224</id>
    </head>
    <body>
        <!-- Terminology -->
        <question type="extended-answer" id="-1">
            <body>What is the difference between a node, socket, core, and a (HW) thread?</body>
            <answers>
                <answer>
                    <body>A node is a collection of hardware connected to other nodes via network. A node has one or
                        multiple physical sockets where processing units are mounted. A core is a part of the processing
                        unit that can execute instructions, of which a processing unit can have multiple. A hardware
                        thread is an execution set (registers, etc.), of which a core can have multiple.
                    </body>
                    <marks>
                        <mark type="keyword">network</mark>
                        <mark type="keyword">unit</mark>
                        <mark type="keyword">instruction</mark>
                        <mark type="keyword">set</mark>
                    </marks>
                </answer>
            </answers>
        </question>


        <!-- From Recap Handwritten Notes on Presentations -->

        <!-- Variability -->
        <question type="extended-answer" id="0">
            <body>What are some hardware/software reasons that running a program on identical hardware can lead to
                different results? What is the consequence?
            </body>
            <answers>
                <answer>
                    <body>Hardware: Same chip can vary under different conditions (such as temperature/thermal
                        throttling). A different chip of the same model can vary due to production differences in the
                        same conditions.
                        <p/>
                        Software: Any resources that are shared (cpu, network, etc.) can be contended, OS-Scheduling and
                        energy-saving features, etc.
                        <p/>
                        Consequence: Don't expect same performance between any 2 different runs, treat runs as a
                        stochastic experiment. Repeat it as many times as necessary.
                    </body>
                    <marks>
                        <mark type="keyword">temp</mark>
                        <mark type="keyword">sched</mark>
                        <mark type="keyword">experiment</mark>
                        <mark type="keyword">rep</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="1">
            <body>How can variability be mitigated?</body>
            <answers>
                <answer>
                    <body>
                        Work with data stochastically
                        <p/>
                        Conduct multiple experiments
                        <p/>
                        Measure variance/confidence interval
                        <p/>
                        Try to control as much of the environment as possible
                        <p/>
                        Ensure there is a good spread over uncontrollable environment influences (measure on different
                        days, etc.)
                        <p/>
                        Be skeptical about results, look for unexpected changes in the environment.
                    </body>
                    <marks>
                        <mark type="keyword">stochast</mark>
                        <mark type="keyword">experiment</mark>
                        <mark type="keyword">environment</mark>
                        <mark type="keyword">control</mark>
                    </marks>
                </answer>
            </answers>
        </question>

        <!-- Overhead and Intrusion -->

        <question type="extended-answer" id="2">
            <body>What does Score-P do?</body>
            <answers>
                <answer>
                    <body>TODO</body>
                    <marks></marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="3">
            <body>What do you need to look out for when using Score-P? How can you identify this?</body>
            <answers>
                <answer>
                    <body>Unexpected or unintended flushes of the log file from memory to permanent storage, which cause
                        overhead and affect code execution.
                        <p/>
                        Identify overhead by looking at the execution time.
                    </body>
                    <marks>
                        <mark type="keyword">flush</mark>
                        <mark type="keyword">overhead</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="4">
            <body>What methods are available to reduce the log size in Score-P?</body>
            <answers>
                <answer>
                    <body>Compile time filtering, selective region filter (user regions), switching tracing on/off.
                    </body>
                    <marks>
                        <mark type="keyword">compile</mark>
                        <mark type="keyword">region</mark>
                        <mark type="keyword">filter</mark>
                        <mark type="keyword">trac</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="5">
            <body>What are selective region filters (user regions) in Score-P?</body>
            <answers>
                <answer>
                    <body>TODO</body>
                    <marks></marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="6">
            <body>What is the problem with runtime filtering? How can it be minimized?</body>
            <answers>
                <answer>
                    <body>There still needs to be a check whether this event should be logged. Good implementations can
                        patch this out by running an uninstrumented version of the code instead. However, some amount of
                        overhead remains.
                    </body>
                    <marks>
                        <mark type="keyword">check</mark>
                        <mark type="keyword">patch</mark>
                        <mark type="keyword">overhead</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="7">
            <body>What are some pros/cons for compile time vs runtime filtering? How are these filters used?</body>
            <answers>
                <answer>
                    <body>Compile time gets rids of all overhead that is unnecessary (because of filtering, etc.). It is
                        set by passing a flag to scorep_compile with the filter file.
                        <p/>
                        Runtime filtering allows for easier change of the filter and does not require recompilation of
                        the source. It is set by setting an environment variable.
                    </body>
                    <marks>
                        <mark type="keyword">overhead</mark>
                        <mark type="keyword">filter</mark>
                        <mark type="keyword">flag</mark>
                        <mark type="keyword">compil</mark>
                        <mark type="keyword">variable</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="8">
            <body>How can one estimate the memory requirements of a run through scorep? What is this useful for?</body>
            <answers>
                <answer>
                    <body>Using the bundled tool scorep_score, which shows you the memory requirements and warns about
                        potential flushes. It allows you to optimize your filters for memory requirements without
                        actually running your instrumented code (TODO: verify this is actually true).
                    </body>
                    <marks>
                        <mark type="keyword">score</mark>
                    </marks>
                </answer>
            </answers>
        </question>

        <!-- More Terminology -->

        <question type="extended-answer" id="9">
            <body>What is the difference between tracing and profiling? What are some benefits and drawbacks of each?
            </body>
            <answers>
                <answer>
                    <body>Tracing records information on every single call, while profiling only shows aggregate
                        information (e.g. how often a function was called, etc.). Tracing can be used to inspect
                        individual problematic calls but generally creates more overhead than profiling, which is less
                        detailed and therefore also generates less overhead.
                    </body>
                    <marks>
                        <mark type="keyword">call</mark>
                        <mark type="keyword">aggregat</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="10">
            <body>What is sampling and instrumentation? What are some benefits and drawbacks of
                each?
            </body>
            <answers>
                <answer>
                    <body>A sampling based approach halts the program periodically to analyze the program's current
                        state. An instrumentation approach modifies the program to add additional code, for example at
                        the start and end of a method, that performs measurements itself.
                        <p/>
                        Sampling does not require modification of the code/binary and can even be adjusted at runtime.
                        However, it is a stochastic approach and can "miss" calls to functions and other events.
                        Instrumentation, on the other hand, does not miss anything. However, it usually needs more
                        fine-tuning to get the level of overhead to an acceptable level without loosing important
                        details.
                    </body>
                    <marks>
                        <mark type="keyword">measure</mark>
                        <mark type="keyword">overhead</mark>
                        <mark type="keyword">stochast</mark>
                        <mark type="keyword">modif</mark>
                    </marks>
                </answer>
            </answers>
        </question>

        <!-- Single Node Performance -->

        <question type="extended-answer" id="500">
            <body>How can a programmer determine the influence of the compiler on a program's performance?</body>
            <answers>
                <answer>
                    <body>It is possible to run performance experiments using several different sets of compiler
                        settings (or even several different compiler versions, vendors). For further analysis, compilers
                        can generate an optimization report which show performed optimizations alongside the source
                        code.
                    </body>
                    <marks>
                        <mark type="regex">experiment|measure</mark>
                        <mark type="keyword">optimiz</mark>
                        <mark type="keyword">report</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="11">
            <body>What should you look for in a compiler's optimization report?</body>
            <answers>
                <answer>
                    <body>Hints about vectorization, data dependencies and memory access patterns.</body>
                    <marks>
                        <mark type="keyword">vector</mark>
                        <mark type="keyword">dependenc</mark>
                        <mark type="keyword">access</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="12">
            <body>What is a problem that can occur with tools that only see source code and binary?</body>
            <answers>
                <answer>
                    <body>The tool might not be able to correlate places in the binary with source code if there are
                        compiler optimizations that prevent doing so.
                    </body>
                    <marks>
                        <mark type="keyword">compiler</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="13">
            <body>Give an example of how you can give hints to the compiler to allow it to optimize effectively.</body>
            <answers>
                <answer>
                    <body>Using the flag -fno-alias or #pragma iv_dep, the programmer guarantees that there is no
                        aliasing, and therefore no potential overlap of memory regions, of variables in the marked code.
                        This allows further optimization as there are fewer dependencies.
                    </body>
                    <marks>
                        <mark type="keyword">alias</mark>
                        <mark type="keyword">overlap</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="14">
            <body>What two surprising insights were found by the single node performance group?</body>
            <answers>
                <answer>
                    <body>Switching the intel compiler version from 17 to 19 has a dramatic impact on performance using
                        the same compiler settings.
                        <p/>
                        The input size to the program affected the effect of different compiler flags.
                    </body>
                    <marks>
                        <mark type="keyword">version</mark>
                        <mark type="keyword">input</mark>
                    </marks>
                </answer>
            </answers>
        </question>

        <!-- Weak and Strong Scaling -->

        <question type="extended-answer" id="15">
            <body>What is the difference between strong scaling and weak scaling?</body>
            <answers>
                <answer>
                    <body>Strong scaling keeps the problem size constant and changes the number of processes working on
                        the problem, while weak scaling keeps a constant problem size per process by scaling the total
                        problem size with the number of processes.
                    </body>
                    <marks>
                        <mark type="keyword">size</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="16">
            <body>What is an alternative name for weak scaling and why?</body>
            <answers>
                <answer>
                    <body>Memory constrained scaling, because it makes better use of available per node memory resources
                        such as cache and memory bandwidth that scale with the number of processes than strong scaling
                        does.
                    </body>
                    <marks>
                        <mark type="keyword">memory</mark>
                        <mark type="keyword">resource</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="17">
            <body>What problem can occur when setting up a weak scaling experiment?</body>
            <answers>
                <answer>
                    <body>The program runtime might be non-linear with the input size. Assume you have a program that
                        scales in O(n^3), then doubling the number of processes and the input size would result in a
                        problem 8 times as large, or 4 times more per process.
                    </body>
                    <marks>
                        <mark type="keyword">linear</mark>
                        <mark type="keyword">size</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="18">
            <body>What is the formula for weak scaling?</body>
            <answers>
                <answer>
                    <body>TODO</body>
                    <marks></marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="19">
            <body>What is an advantage of weak scaling over strong scaling?</body>
            <answers>
                <answer>
                    <body>It reduces the impact that communication has for large number of processes by keeping the
                        ratio between communication and computation constant. In strong scaling, computation decreases
                        with number of nodes so communication starts to domniate runtime.
                    </body>
                    <marks>
                        <mark type="keyword">communication</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="20">
            <body>What is the question that is typically answered with a weak scaling test?</body>
            <answers>
                <answer>
                    <body>How much larger of a problem can we solve by scaling this? (This tends to be more relevant to
                        scientific application than &quot;how much faster can I solve this problem?&quot;)
                    </body>
                    <marks>
                        <mark type="keyword">size</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="21">
            <body>What is important when dealing with strong/weak scaling and different benchmarks?</body>
            <answers>
                <answer>
                    <body>Understand what you expect. Some benchmarks might adjust the problem size per node
                        automatically (LULESH) or spawn threads for the entire node automatically (firestarter).
                    </body>
                    <marks>
                        <mark type="keyword">size</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <!-- Affinity / Mapping -->
        <question type="extended-answer" id="22">
            <body>What is affinity and what does granularity specify?</body>
            <answers>
                <answer>
                    <body>Affinity is the distribution of software threads to hardware procs. The granularity specifies
                        where software threads can float (fine: core, socket, spread, etc.) and is set in the
                        KMP_Affinity variable.
                    </body>
                    <marks>
                        <mark type="keyword">thread</mark>
                        <mark type="keyword">proc</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="23">
            <body>Why is process affinity important? What needs to be considered?</body>
            <answers>
                <answer>
                    <body>To make good use of the NUMA architecture (memory controllers, network interconnects, etc.).
                        One needs to considered where data is located (and ensure it is actually located there using
                        e.g. first touch) and who needs to communicate with whom to reduce bottlenecks.
                    </body>
                    <marks>
                        <mark type="keyword">numa</mark>
                        <mark type="keyword">touch</mark>
                        <mark type="keyword">communicat</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <!-- MPI overhead -->
        <question type="extended-answer" id="24">
            <body>What is the difference between MPIP and PMPI?</body>
            <answers>
                <answer>
                    <body>mpiP is a MPI-profiler that uses the profiling interface of the MPI standard (PMPI) to perform
                        measurements on the MPI behavior of the program.
                    </body>
                    <marks>
                        <mark type="keyword">profil</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="25">
            <body>What information can you gain from analyzing your program with mpiP?</body>
            <answers>
                <answer>
                    <body>mpiP generates a text report showing the relevant MPI calls (with context), the time spent and
                        the amount of data transferred. This is available on a per node basis (for load imbalances) as
                        well as in aggregated form.
                    </body>
                    <marks>
                        <mark type="keyword">call</mark>
                        <mark type="keyword">time</mark>
                        <mark type="keyword">data</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="26">
            <body>What mechanism does the PMPI interface use to allow profilers to link in?</body>
            <answers>
                <answer>
                    <body>All MPI calls are made available as weak symbols, which can be overriden by the profiler.
                        These can be used to wrap the necessary calls in instrumentation.
                    </body>
                    <marks>
                        <mark type="keyword">weak</mark>
                        <mark type="keyword">symbol</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="27">
            <body>How can a program be run with mpiP?</body>
            <answers>
                <answer>
                    <body>By either linking against the profiler or using LD_Preload, which loads the profiler into the
                        application at runtime.
                    </body>
                    <marks>
                        <mark type="keyword">preload</mark>
                        <mark type="keyword">link</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="28">
            <body>What can you learn from profiling your code using mpiP? What is this useful for?</body>
            <answers>
                <answer>
                    <body>Time and data sizes (hot code regions, also on a per node base). Useful as a first step to
                        investigate potential problems (load imbalance, etc.)
                    </body>
                    <marks>
                        <mark type="keyword">time</mark>
                        <mark type="keyword">size</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="29">
            <body>What can you change in your application if you notice that your code hangs on some MPI calls?</body>
            <answers>
                <answer>
                    <body>Change the application to support nonblocking communication (i.e. do useful computation rather
                        than wait for communication resources).
                    </body>
                    <marks>
                        <mark type="keyword">block</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="30">
            <body>You observe that in your MPI application with 4 processes, rank 1 spends 32%, rank 2 28%, rank 3 11%,
                rank 4 31% of the total time in MPI calls. Which rank is likely to be overloaded?
            </body>
            <answers>
                <answer>
                    <body>Rank 3 (the rank which spends most time computing and little time waiting).</body>
                    <marks>
                        <mark type="keyword">3</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <!-- Load Balancing -->
        <question type="extended-answer" id="31">
            <body>Why does measurement in OpenMP require restructuring of the loop?</body>
            <answers>
                <answer>
                    <body>Cannot measure implicit barriers, need to convert into explicit barriers (tools like scoreP do
                        this automatically). Then measure waiting time at synchronization points.
                    </body>
                    <marks>
                        <mark type="keyword">barrier</mark>
                        <mark type="keyword">synchron</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="32">
            <body>What tools are useful for measuring load balancing with OpenMP and what do they do?</body>
            <answers>
                <answer>
                    <body>Automatic instrumentation: ScoreP/Opari. Visualization: Ale (TODO?)</body>
                    <marks>
                        <mark type="keyword">instrument</mark>
                        <mark type="keyword">score</mark>
                        <mark type="keyword">opari</mark>
                        <mark type="keyword">visual</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="33">
            <body>What is the formula for optimization potential?</body>
            <answers>
                <answer>
                    <body>
                        Imbalance time:
                        <p/>
                        User code: t_imbalance = t_max - t_avg
                        <p/>
                        Synchronization: t_imbalance = t_min - t_avg
                        <p/>
                        Imbalance ratio: r_imbalance = (t_imbalance / t_max) * (N / N-1)
                    </body>
                    <marks>
                        <mark type="keyword">time</mark>
                        <mark type="keyword">ratio</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="34">
            <body>What advantage/disadvantage does Vampir offer for visualization?</body>
            <answers>
                <answer>
                    <body>Tons of detail to find even small problems, but can require lots of zooming and searching to
                        find the things you are looking for (information overload).
                    </body>
                    <marks>
                        <mark type="keyword">detail</mark>
                        <mark type="keyword">zoom</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="35">
            <body>In general, what is a typical cause of load imbalance?</body>
            <answers>
                <answer>
                    <body>Anything that causes global synchronization.</body>
                    <marks>
                        <mark type="keyword">sync</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <!-- Power + Energy -->
        <question type="extended-answer" id="36">
            <body>Which problems lend themselves well to higher frequency?</body>
            <answers>
                <answer>
                    <body>Compute intensive ones.</body>
                    <marks>
                        <mark type="keyword">comput</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="37">
            <body>How does core frequency affect power/energy consumption?</body>
            <answers>
                <answer>
                    <body>A higher core frequency implies higher power consumption, but may mean higher or lower energy
                        consumption.
                    </body>
                    <marks>
                        <mark type="keyword">power</mark>
                        <mark type="keyword">energy</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="38">
            <body>What is static and dynamic power? What is the relation of power consumption to frequency?</body>
            <answers>
                <answer>
                    <body>
                        Static power: TODO? (Something like P_static = VI_leak)
                        <p/>
                        Dynamic power: P_dyn=CfV^2
                        <p/>
                        Maximum frequency: f \correlates (V-V_th)^(\alpha)/V
                        <p/>
                        In general: Power increases with the cube of frequency.
                    </body>
                    <marks>
                        <mark type="keyword"></mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="39">
            <body>What is important when considering power/energy consumption optimization?</body>
            <answers>
                <answer>
                    <body>Think about what to optimize against and what constraints exist (cables, money spent, etc.)
                    </body>
                    <marks>
                        <mark type="keyword">optim</mark>
                        <mark type="keyword">constraint</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="40">
            <body>What is the difference between Watt, Watt hours, and Joules?</body>
            <answers>
                <answer>
                    <body>Watt is an SI Unit of Power (a rate of energy) J/s. Watt hours is a non SI measure of Energy
                        (J/s * 3600s = 3600J). Joule is the SI measure of energy.
                    </body>
                    <marks>
                        <mark type="keyword">power</mark>
                        <mark type="keyword">energy</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <!-- Dynamism -->
        <question type="extended-answer" id="41">
            <body>What is dynamism?</body>
            <answers>
                <answer>
                    <body>Execution behavior that varies over the run of a program, such as across iterations or through
                        different calculation phases.
                    </body>
                    <marks>
                        <mark type="keyword">vari</mark>
                        <mark type="keyword">iterat</mark>
                        <mark type="keyword">phase</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="42">
            <body>How can dynamism across different phases be measured?</body>
            <answers>
                <answer>
                    <body>One possibility is to introduce user regions into the source to measure the phases
                        separately.
                    </body>
                    <marks>
                        <mark type="keyword">user</mark>
                        <mark type="keyword">region</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="43">
            <body>What are some characteristics where dynamism can occur?</body>
            <answers>
                <answer>
                    <body>Execution time, cache miss rate, etc.</body>
                    <marks>
                        <mark type="keyword">time</mark>
                        <mark type="keyword">cache</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="44">
            <body>What tool is useful to track dynamism? What features are relevant?</body>
            <answers>
                <answer>
                    <body>Its CUBE! Dynamic region profiling (TODO: look this up?) and parameter based profiling are
                        relevant.
                    </body>
                    <marks>
                        <mark type="keyword">CUBE</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="45">
            <body>What is parameter based profiling? What is it useful for?</body>
            <answers>
                <answer>
                    <body>A feature of CUBE that allows to aggregate data based on a parameter such as the loop
                        iteration. It allows the user to correlate dynamism with the parameter.
                    </body>
                    <marks>
                        <mark type="keyword">param</mark>
                        <mark type="keyword">dynamism</mark>
                    </marks>
                </answer>
            </answers>
        </question>

        <!-- Lectures -->
        <!-- Introduction -->
        <question type="extended-answer" id="46">
            <body>What are some of the motivations for parallel architectures?</body>
            <answers>
                <answer>
                    <body>Performance, energy efficiency, flexibility, throughput, fault-tolerance</body>
                    <marks>
                        <mark type="keyword">perform</mark>
                        <mark type="keyword">energy</mark>
                        <mark type="keyword">flex</mark>
                        <mark type="keyword">throughput</mark>
                        <mark type="keyword">fault</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="47">
            <body>What are some usages of parallel architectures and what do they mean?</body>
            <answers>
                <answer>
                    <body>Multiple user workloads (many users on one machine), multi-programming workloads, farming (run
                        lots of sequential jobs), parallel execution.
                    </body>
                    <marks>
                        <mark type="keyword">user</mark>
                        <mark type="keyword">program</mark>
                        <mark type="keyword">farm</mark>
                        <mark type="keyword">execut</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="48">
            <body>What is farming, what potential problem can arise?</body>
            <answers>
                <answer>
                    <body>Running lots of sequential jobs, varying them over some parameters. It can create issues with
                        the scheduler if thousands of jobs are spawned.
                    </body>
                    <marks>
                        <mark type="keyword">sequen</mark>
                        <mark type="keyword">schedul</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="49">
            <body>What are some parallel programming models?</body>
            <answers>
                <answer>
                    <body>Message passing, shared memory, data parallel (OpenCL, CUDA), data flow.</body>
                    <marks>
                        <mark type="keyword">message</mark>
                        <mark type="keyword">shared</mark>
                        <mark type="keyword">data</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="50">
            <body>What are some reasons why parallel programming is more difficult than sequential programming?</body>
            <answers>
                <answer>
                    <body>Management of threads/processes, concurrency issues (race conditions/deadlocks), less
                        deterministic, ensuring scalability.
                    </body>
                    <marks>
                        <mark type="keyword">process</mark>
                        <mark type="keyword">concurren</mark>
                        <mark type="keyword">determin</mark>
                        <mark type="keyword">scalab</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="51">
            <body>What is speedup, how is it calculated? What is efficiency, how is it calculated?</body>
            <answers>
                <answer>
                    <body>Speedup is a ratio of performance (performance (p processors) / performance (1 processor)).
                        Using performance = work/time: speedup = time(1 processor)/time(p processors). Efficiency is the
                        relation of actual speedup to ideal linear scaling: efficiency = speedup(p processors)/p.
                    </body>
                    <marks>
                        <mark type="keyword">perform</mark>
                        <mark type="keyword">time</mark>
                        <mark type="keyword">linear</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="52">
            <body>Which distributed model does the SuperMUC-NG conform to?</body>
            <answers>
                <answer>
                    <body>Intra-Node: Shared Memory. Inter-Node: Distributed Memory (message passing).</body>
                    <marks>
                        <mark type="keyword">share</mark>
                        <mark type="keyword">distribut</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="53">
            <body>What is the difference between Infiniband and OmniPath Networks?</body>
            <answers>
                <answer>
                    <body>Infiniband tries to offload as much processing as possible to the network card while OmniPath
                        (Intel) tries to onload as much as possible. Both Range near 100 Gb/s (approx.?)
                    </body>
                    <marks>
                        <mark type="keyword">offload</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="54">
            <body>What does the SuperMUC use as intra-/inter- island topology?</body>
            <answers>
                <answer>
                    <body>Intra island: Non-blocking tree (Multiple pairwise communications in parallel). Inter island:
                        pruned tree (like a fat tree, but grow bandwidth close to root slower than fat tree).
                    </body>
                    <marks>
                        <mark type="keyword">block</mark>
                        <mark type="keyword">prun</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="55">
            <body>What is PUE and ERE?</body>
            <answers>
                <answer>
                    <body>Power usage effectiveness (ratio of total facility power / power used by IT equipment!) (ideal
                        1, lower is better). Energy reuse effectiveness: (Total facility power - reused power/IT
                        equipment power) (ideal 0.0, range 0.0-ERE).
                    </body>
                    <marks>
                        <mark type="keyword">power</mark>
                        <mark type="keyword">usage</mark>
                        <mark type="keyword">effective</mark>
                        <mark type="keyword">reuse</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="56">
            <body>What are some SuperMUC optimizations for energy savings?</body>
            <answers>
                <answer>
                    <body>Water cooling (no fan per node), warm water cooling, heat reuse, energy aware scheduling.
                    </body>
                    <marks>
                        <mark type="keyword">water</mark>
                        <mark type="keyword">warm</mark>
                        <mark type="keyword">reuse</mark>
                        <mark type="keyword">schedul</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="57">
            <body>What architecture does automatic parallelization work well with? Where do problems start to arise?
            </body>
            <answers>
                <answer>
                    <body>Automatic parallelization can work well on shared memory systems for specific applications. It
                        struggles with NUMA, because its harder to place data correctly.
                    </body>
                    <marks>
                        <mark type="keyword">shared</mark>
                        <mark type="keyword">memory</mark>
                        <mark type="keyword">NUMA</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="58">
            <body>Aside from concurrency bugs (race conditions and deadlocks), what other source of non determinism was
                covered in the lecture?
            </body>
            <answers>
                <answer>
                    <body>Reductions on floating point values, where the result depends on the order of operations.
                    </body>
                    <marks>
                        <mark type="keyword">float</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="59">
            <body>What is the probable reason for super-linear speedup?</body>
            <answers>
                <answer>
                    <body>Better cache efficiency, probably because there is more total cache or better locality.</body>
                    <marks>
                        <mark type="keyword">cache</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="60">
            <body>How are cores arranged on the SuperMUC Phase 2 Processors?</body>
            <answers>
                <answer>
                    <body>In two rings with two interconnects.</body>
                    <marks>
                        <mark type="keyword">ring</mark>
                        <mark type="keyword">connect</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="61">
            <body>What is a resource problem inherent in MPI that OpenMP does not have? Why is this a problem?</body>
            <answers>
                <answer>
                    <body>In MPI (parts of) data structures and code are duplicated. This is a problem, because the
                        trend goes toward less memory/core.
                    </body>
                    <marks>
                        <mark type="keyword">duplicat</mark>
                        <mark type="keyword">data</mark>
                        <mark type="keyword">code</mark>
                        <mark type="keyword">memory</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="62">
            <body>What are the levels of thread safety that an MPI implementation can offer you?</body>
            <answers>
                <answer>
                    <body>Single (No additional threads in system), funneled (MPI calls only in master thread),
                        serialized (only one thread runs MPI calls at any time), parallel/multiple (any thread, any
                        time)
                    </body>
                    <marks>
                        <mark type="keyword">single</mark>
                        <mark type="keyword">funnel</mark>
                        <mark type="keyword">serial</mark>
                        <mark type="regex" regex-flags="i">parallel|multiple</mark>
                    </marks>
                </answer>
            </answers>
        </question>

        <!-- Parallel Patterns -->
        <question type="extended-answer" id="63">
            <body>What is task decomposition and what needs to be considered?</body>
            <answers>
                <answer>
                    <body>Decomposition of problem into tasks that are processed in parallel.
                        <p/>
                        Consider: Number, granularity, parallelism, resource contention and dependencies.
                    </body>
                    <marks>
                        <mark type="keyword">granular</mark>
                        <mark type="keyword">dependenc</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="64">
            <body>What is data decomposition and what needs to be considered?</body>
            <answers>
                <answer>
                    <body>Decomposition of the problem domain into blocks that can be processed in parallel. Consider:
                        Domain-size, load balancing, access patterns, dependencies, communication.
                    </body>
                    <marks>
                        <mark type="keyword">domain</mark>
                        <mark type="keyword">communcation</mark>
                        <mark type="keyword">balanc</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="65">
            <body>What is independent task execution? Which decomposition does it apply to? (algorithmic structure)
            </body>
            <answers>
                <answer>
                    <body>Individual execution of tasks under consideration of dependencies and resource contention.
                        Applies to task-based decompositions.
                    </body>
                    <marks>
                        <mark type="keyword">execut</mark>
                        <mark type="keyword">dependen</mark>
                        <mark type="keyword">resource</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="66">
            <body>What is aggregation of tasks and in which context is it used? What problem can arise? (algorithmic
                structure)
            </body>
            <answers>
                <answer>
                    <body>Combining several tasks from the original decomposition into larger super-tasks to reduce the
                        scheduling overhead. Can run into load balance problems if too coarse.
                    </body>
                    <marks>
                        <mark type="regex">combin|collaps</mark>
                        <mark type="keyword">task</mark>
                        <mark type="keyword">balanc</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="67">
            <body>What is the recursive tasks pattern and in what context can it be used? How can overhead be reduced?
                (algorithmic structure)
            </body>
            <answers>
                <answer>
                    <body>A pattern that lends itself well to recursive data structures, where problems spawn
                        subproblems which are then reduced into the output (divide and conquer). Consider a threshold
                        below of which no further subproblems are spawned to reduce overhead.
                    </body>
                    <marks>
                        <mark type="keyword">recurs</mark>
                        <mark type="keyword">sub</mark>
                        <mark type="keyword">thresh</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="68">
            <body>What is static task scheduling, some advantages and disadvantages?</body>
            <answers>
                <answer>
                    <body>A scheduling where all task assignments are fixed ahead of time. Works with very little
                        communication and synchronization, but can lead to significant load imbalance if used
                        incorrectly.
                    </body>
                    <marks>
                        <mark type="keyword">fix</mark>
                        <mark type="keyword">communicat</mark>
                        <mark type="keyword">balanc</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="69">
            <body>What is dynamic task scheduling, some advantages and disadvantages?</body>
            <answers>
                <answer>
                    <body>A scheduling that allows changes during runtime and dynamic load balancing. Has more overhead
                        than static task scheduling but can deal with load imbalance.
                    </body>
                    <marks>
                        <mark type="keyword">overhead</mark>
                        <mark type="keyword">balanc</mark>
                    </marks>
                </answer>
            </answers>
        </question>

        <!-- ... -->
        <!-- Data decomposition -->
        <question type="extended-answer" id="70">
            <body>What are the 5 data decomposition patterns?</body>
            <answers>
                <answer>
                    <body>Static, redistribution, irregular (using a partitioning algorithm based on load and
                        adjacencies), regular, oversubscription (cannot account for number of execution units).
                    </body>
                    <marks>
                        <mark type="keyword">static</mark>
                        <mark type="keyword">redistribut</mark>
                        <mark type="keyword">irregular</mark>
                        <mark type="keyword">regular</mark>
                        <mark type="keyword">oversubscri</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <!-- ... -->

        <question type="extended-answer" id="71">
            <body>What are some typical performance problems that occur in sequential code?</body>
            <answers>
                <answer>
                    <body>Lack of (automatic) vectorization. Access problems in cache, TLB, or just memory bandwidth.
                        Contention of other resources like IO, paging, daemons or memory allocation.
                    </body>
                    <marks>
                        <mark type="keyword">vector</mark>
                        <mark type="keyword">memory</mark>
                        <mark type="keyword">conten</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="72">
            <body>What are some typical performance problems in shared memory systems?</body>
            <answers>
                <answer>
                    <body>Load imbalance, false sharing, lock contention, thread library overhead, NUMA.</body>
                    <marks>
                        <mark type="keyword">balanc</mark>
                        <mark type="keyword">shar</mark>
                        <mark type="keyword">lock</mark>
                        <mark type="keyword">thread</mark>
                        <mark type="keyword">NUMA</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="73">
            <body>What are some typical performance problems in distributed memory systems?</body>
            <answers>
                <answer>
                    <body>Load imbalance, excessive collectives, network contention and process placement.</body>
                    <marks>
                        <mark type="keyword">balanc</mark>
                        <mark type="keyword">collectiv</mark>
                        <mark type="keyword">net</mark>
                        <mark type="keyword">placement</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="74">
            <body>What are the goals of HPCToolkit?</body>
            <answers>
                <answer>
                    <body>Performance analysis of application, without need for source, with full optimization support
                        and support for sequential, OpenMP, MPI.
                    </body>
                    <marks>
                        <mark type="keyword">perform</mark>
                        <mark type="keyword">source</mark>
                        <mark type="keyword">optimiz</mark>
                        <mark type="keyword">openmp</mark>
                        <mark type="keyword">mpi</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="75">
            <body>What is the workflow with HPCToolkit?</body>
            <answers>
                <answer>
                    <body>
                        Using optimized binary:
                        <p/>
                        Binary analysis: hpcstruct ...
                        <p/>
                        Execution with profiling: [mpi-launcher] hpcrun [hpcrun-options] app [app-arguments]
                        <p/>
                        Interpret profile, correlate [w/ source]: hpcprof -S app.hpcstruct -I [app-src]/’*’ \
                        hpctoolkit-app-measurements1
                        <p/>
                        Inspect profile with hpcviewer.
                    </body>
                    <marks>
                        <mark type="keyword">hpcstruct</mark>
                        <mark type="keyword">hpcrun</mark>
                        <mark type="keyword">hpcprof</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="76">
            <body>What are ways to reduce overhead in HPCToolkit?</body>
            <answers>
                <answer>
                    <body>Using flag -f 0.1 (monitor 1/10th of all processes) or using
                        hpctoolkit_sampling_start()/stop().
                    </body>
                    <marks>
                        <mark type="keyword">-f</mark>
                        <mark type="keyword">start</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="77">
            <body>What are the three views in HPCToolkit and what do they show?</body>
            <answers>
                <answer>
                    <body>Calling context (top-down), caller (bottom-up), flat (rank by metric).</body>
                    <marks>
                        <mark type="keyword">call</mark>
                        <mark type="keyword">context</mark>
                        <mark type="keyword">flat</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="78">
            <body>How do you define derived metrics in hpcviewer?</body>
            <answers>
                <answer>
                    <body>Using the formula window, which allows you to use other metrics as parameters: $1 (pointwise)
                        or @2 (aggregated).
                    </body>
                    <marks>
                        <mark type="keyword">formula</mark>
                        <mark type="keyword">pointwise</mark>
                        <mark type="keyword">aggregated</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <!-- CUBE -->
        <question type="extended-answer" id="79">
            <body>What does CUBE stand for and what are the 3 axes of the cube?</body>
            <answers>
                <answer>
                    <body>CUBE Uniform Behavioral Encoding. Performance (metric) pane, call path pane, location pane
                        (rank, thread).
                    </body>
                    <marks>
                        <mark type="keyword">CUBE Uniform Behavioral Encoding</mark>
                        <mark type="regex">performance|metric</mark>
                        <mark type="keyword">call</mark>
                        <mark type="keyword">location</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="80">
            <body>How are inclusive/exclusive values displayed in CUBE's center pane?</body>
            <answers>
                <answer>
                    <body>Collapsed form displays inclusive values and expanded form displays exclusive values.</body>
                    <marks>
                        <mark type="keyword">collapse</mark>
                        <mark type="keyword">expand</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="81">
            <body>What can you analyze with Vampir but not with a profiling tool?</body>
            <answers>
                <answer>
                    <body>The application timeline, individual call traces, individual communication.</body>
                    <marks>
                        <mark type="keyword">timeline</mark>
                        <mark type="keyword">trace</mark>
                        <mark type="keyword">communicat</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="82">
            <body>How does the VampirServer work? What are some advantages of using it?</body>
            <answers>
                <answer>
                    <body>It answers queries from the vampir visualizer, allowing the analysis to run on more powerful
                        hardware and avoiding the need to transfer large trace files.
                    </body>
                    <marks>
                        <mark type="keyword">quer</mark>
                        <mark type="keyword">hardware</mark>
                        <mark type="keyword">file</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="84">
            <body>What is Scalasca used for?</body>
            <answers>
                <answer>
                    <body>Automatic search for inefficient behavior according to patterns.</body>
                    <marks>
                        <mark type="keyword">behavior</mark>
                        <mark type="keyword">pattern</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="85">
            <body>If the application runs on N nodes, how many nodes does scalasca -analyze use?</body>
            <answers>
                <answer>
                    <body>TODO</body>
                    <marks></marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="86">
            <body>What is the late sender antipattern in MPI?</body>
            <answers>
                <answer>
                    <body>An antipattern where a blocking receive operation is posted before the corresponding send is
                        executed, causing the receiver to wait on the sender.
                    </body>
                    <marks>
                        <mark type="regex">block|wait</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="87">
            <body>What is the early root reduce antipattern in MPI?</body>
            <answers>
                <answer>
                    <body>A waiting time that occurs in N-to-1 communication (Reduce, Gather, Gatherv, etc.) if the
                        target (receiver) starts the operation before the senders.
                    </body>
                    <marks>
                        <mark type="keyword">wait</mark>
                    </marks>
                </answer>
            </answers>
        </question>

        <!-- Miscellaneous -->
        <question type="extended-answer" id="999">
            <body>What is CUBE not good for?</body>
            <answers>
                <answer>
                    <body>TODO. (Problem-domain based visualization).</body>
                    <marks></marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="83">
            <body>What tools does Score-P consist of and what are they used for?</body>
            <answers>
                <answer>
                    <body>Vampir (tracing/visualization), Scalasca for automatic antipattern detection (with CUBE?), TAU
                        for generic instrumentation, Periscope (online measurements)
                    </body>
                    <marks>
                        <mark type="keyword">vampir</mark>
                        <mark type="keyword">visual</mark>
                        <mark type="keyword">scalasca</mark>
                        <mark type="keyword">TAU</mark>
                        <mark type="keyword">instrument</mark>
                        <mark type="keyword">periscope</mark>
                        <mark type="keyword">online</mark>
                    </marks>
                </answer>
            </answers>
        </question>

        <!-- Additional questions -->
        <question type="extended-answer" id="1000">
            <body>What is NUMA?</body>
            <answers>
                <answer>
                    <body>Non Uniform Memory Access: A type of shared memory system that allows for a non-uniform memory
                        hierarchy. Certain memory regions can be closer to a node than other regions.
                    </body>
                    <marks>
                        <mark type="keyword">Non Uniform Memory Access</mark>
                        <mark type="keyword">shared</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="1001">
            <body>What are the NUMA domains in the Haswell based nodes?</body>
            <answers>
                <answer>
                    <body>The haswell based nodes have 4 NUMA domains including hyperthreads. The grouping is as follows:<p/>
                    Domain: 0 Processors: (0 28 1 29 2 30 3 31 4 32 5 33 6 34) <p/>
                    Domain: 1 Processors: (7 35 8 36 9 37 10 38 11 39 12 40 13 41) <p/>
                    Domain: 2 Processors: (14 42 15 43 16 44 17 45 18 46 19 47 20 48) <p/>
                    Domain: 4 Processors: (21 49 22 50 23 51 24 52 25 53 26 54 27 55) <p/>
                    </body>
                    <marks></marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="1002">
            <body>What is the role of SIMD units in the SuperMUC-NG?</body>
            <answers>
                <answer>
                    <body>3 SIMD units per core in Skylake-EP. The purpsose is to execute vector instructions based on AVX (AVX-512, AVX-2) ISA so 
                    as to increase performance (FLOPS) of the code. 
                    </body>
                    <marks></marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="1003">
            <body>What is the difference between blocking and nonblocking operations?</body>
            <answers>
                <answer>
                    <body>Blocking: Wait until buffer can be reused, Non-blocking: Return as fast as possible and query
                        for completion later.
                    </body>
                    <marks>
                        <mark type="keyword">wait</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="1004">
            <body>What are different work sharing constructs in OpenMP?</body>
            <answers>
                <answer>
                    <body>Sections, for loops, single and master.</body>
                    <marks></marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="1005">
            <body>What are different types of instrumentation?</body>
            <answers>
                <answer>
                    <body>TODO. (Source-2-Source, Compile-time, Binary)?</body>
                    <marks></marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="1006">
            <body>How can you optimize for a TLB?</body>
            <answers>
                <answer>
                    <body>TODO</body>
                    <marks></marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="1007">
            <body>What is the special challenge posed when analyzing multigrid programs?</body>
            <answers>
                <answer>
                    <body>TODO</body>
                    <marks></marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="1008">
            <body>What is the difference between instrumentation, monitoring and analysis?</body>
            <answers>
                <answer>
                    <body>TODO</body>
                    <marks></marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="1009">
            <body>How are threads handled in HPCToolkit in relation to OMP?</body>
            <answers>
                <answer>
                    <body>TODO</body>
                    <marks></marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="1010">
            <body>What is computational imbalance in scalasca?</body>
            <answers>
                <answer>
                    <body>TODO</body>
                    <marks></marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="1011">
            <body>What is MPI?</body>
            <answers>
                <answer>
                    <body>A standardized specification of an API used for message passing in parallel applications.
                    </body>
                    <marks>
                        <mark type="keyword">standard</mark>
                        <mark type="keyword">spec</mark>
                        <mark type="keyword">api</mark>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="1012">
            <body>Advantage of using a MESH based network architecture in Skylake?</body>
            <answers>
                <answer>
                    <body>More scalable as comapared to partitioned ring interconnect architecture in Haswell-EP.
                    </body>
                    <marks>
                    </marks>
                </answer>
            </answers>
        </question>
                <question type="extended-answer" id="1013">
            <body>What is Sub Numa Clustering (SNC)?</body>
            <answers>
                <answer>
                    <body>Skylake-EP has a memory controller located on each side of the die. SNC allows for creation of localized domains with each
                    memory controller belonging to each domain. It leads to low LLC and memory latency within its domain compared to accesses outside the 
                    domain.
                    </body>
                    <marks>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="2001">
            <body>What is Processor affinity?</body>
            <answers>
                <answer>
                    <body>Cache Affinity enables the binding and unbinding of a process or a thread to CPU or a range of CPUs, so that the process or 
                          thread will execute only on the designated CPU or CPUs rather than any CPU. This can be viewed as a modification of the 
                          native central queue scheduling algorithm in a symmetric multiprocessing operating system. 
                    </body>
                    <marks>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="2002">
            <body>What is NUMA nodes?</body>
            <answers>
                <answer>
                    <body>Non-uniform memory access (NUMA) is a computer memory design used in multiprocessing, where the memory access time depends on 
                          the memory location relative to the processor. Under NUMA, a processor can access its own local memory faster than non-local 
                          memory (memory local to another processor or memory shared between processors). The benefits of NUMA are limited to particular 
                          workloads, notably on servers where the data is often associated strongly with certain tasks or users.
                    </body>
                    <marks>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="1014">
            <body>What are the drawbacks of worksharing in OpenMP?</body>
            <answers>
                <answer>
                    <body>
                    Imbalance caused by the workload, imbalance caused by the machine (due to heterogeneity), limited programming flexibility.
                    </body>
                    <marks>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="1015">
            <body>What are tasks in OpenMP and how are they scheduled?</body>
            <answers>
                <answer>
                    <body>
                    Tasks are independent pieces of work, which are guaranteed to be executed in any order. Scheduling is done by the runtime system 
                    using task queues.
                    </body>
                    <marks>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="1016">
            <body>What are performance considerations while using explicit tasking with OpenMP?</body>
            <answers>
                <answer>
                    <body>
                    Task Granularity, NUMA Optimization (Tasks can be executed anywhere).
                    </body>
                    <marks>
                    </marks>
                </answer>
            </answers>
        </question>
        <question type="extended-answer" id="1017">
            <body>What is the difference between MPI_Isend() and MPI_Issend()?</body>
            <answers>
                <answer>
                    <body>
                    Both the routines return immediately. The difference is when the MPI library signals to the user that the send buffer 
                    can be reused (MPI_Wait and MPI_Test returns). In case of MPI_Isend it returns when message has been copied to the local buffer and in 
                    case of MPI_Issend it reutrns when matching recieve has been posted.
                    </body>
                    <marks>
                    </marks>
                </answer>
            </answers>
        </question>
    </body>
</exam>
